[{"title":"语法样本","url":"/2021/09/03/sample/","content":"\n<center>语法样本</center>\n<!--more-->\n\n\n## 插入图片\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/b9dddd6c69cdbbba.png\" width=\"300\"/></div>\n\n## 公式\n$$\nE(\\mathcal{T}) \\approx \\sum_{i} \\sum_{j} \\sqrt{\\sum_{d}\\left(\\nabla_{1} \\mathbf{r}_{i, j, d}\\right)^{2}+\\left(\\nabla_{2} \\mathbf{r}_{i, j, d}\\right)^{2}+\\epsilon}\n$$\n\n## 代码\n```\nsudo apt-get autoremove\n```\n\n## 超链接\n[SDK Manager](https://developer.nvidia.com/nvidia-sdk-manager)\n\n## 缩进与空行\n&emsp;&emsp;缩进\n\n&nbsp; \n\n&emsp;&emsp;换行\n\n## 列举\n* 一二三四五\n* 上山打老虎\n* 老虎打不到\n* 打到小松鼠\n\n## 排序\n1. 窗前明月光\n2. 疑是地上霜\n3. 举头望明月\n4. 低头思故乡\n\n\n\n\n\n","tags":["写给CC"],"categories":["网站建设"]},{"title":"ESP8266+电灯科技+天猫精灵","url":"/2021/08/22/8266light/","content":"\n<center>物联网智能家居?</center>\n<!--more-->\n\n&nbsp;\n\n## 硬件\n* ESP 8266 12F WIFI 芯片\n* SG90 舵机*2\n* micro usb 数据线一根\n* 杜邦线若干\n* 充电宝（给芯片供电）\n* 热熔胶（固定舵机等）\n\n## esp8266烧录\n### esp8266 驱动安装\n[CH340驱动安装](https://www.onlinedown.net/soft/1164748.htm), 不然显示不出端口\n### \n\n## \n\n\n\n\n```python\n\n\n#define BLINKER_WIFI\n#define BLINKER_ALIGENIE_OUTLET\n#include <Servo.h>\n \n// #define PIN_SERVO D0  //舵机信号线\n\nServo myservo1;\nServo myservo2;\n \n#include <Blinker.h>\n\nchar auth[] = \"c71c4d5b26c8\";\nchar ssid[] = \"ChinaNet-b7Xx\";\nchar pswd[] = \"essxbfbw\";\n\nbool oState = false;\n\nvoid aligeniePowerState(const String & state)\n{\n    BLINKER_LOG(\"need set power state: \", state);\n\n    myservo1.attach(D0);\n    myservo2.attach(D1);\n    \n    if (state == BLINKER_CMD_ON) {\n        myservo1.write(48);//舵机正转度数\n        myservo2.write(60);\n        delay(1000); \n\n        BlinkerAliGenie.powerState(\"on\");\n        BlinkerAliGenie.print();\n\n        oState = true;\n    }\n    else if (state == BLINKER_CMD_OFF) {\n\n        myservo1.write(-48);//舵机反转度数\n        myservo2.write(-60);\n        delay(1000); \n       \n        BlinkerAliGenie.powerState(\"off\");\n        BlinkerAliGenie.print();\n\n        oState = false;\n    }\n}\n\nvoid aligenieQuery(int32_t queryCode)\n{\n    BLINKER_LOG(\"AliGenie Query codes: \", queryCode);\n\n    switch (queryCode)\n    {\n        case BLINKER_CMD_QUERY_ALL_NUMBER :\n            BLINKER_LOG(\"AliGenie Query All\");\n            BlinkerAliGenie.powerState(oState ? \"on\" : \"off\");\n            BlinkerAliGenie.print();\n            break;\n        case BLINKER_CMD_QUERY_POWERSTATE_NUMBER :\n            BLINKER_LOG(\"AliGenie Query Power State\");\n            BlinkerAliGenie.powerState(oState ? \"on\" : \"off\");\n            BlinkerAliGenie.print();\n            break;\n        default :\n            BlinkerAliGenie.powerState(oState ? \"on\" : \"off\");\n            BlinkerAliGenie.print();\n            break;\n    }\n}\n\nvoid dataRead(const String & data)\n{\n    BLINKER_LOG(\"Blinker readString: \", data);\n\n    Blinker.vibrate();\n    \n    uint32_t BlinkerTime = millis();\n    \n    Blinker.print(\"millis\", BlinkerTime);\n}\n\nvoid setup()\n{\n    Serial.begin(115200);\n    BLINKER_DEBUG.stream(Serial);\n\n    pinMode(LED_BUILTIN, OUTPUT);\n    digitalWrite(LED_BUILTIN, LOW);\n\n    Blinker.begin(auth, ssid, pswd);\n    Blinker.attachData(dataRead);\n    \n    BlinkerAliGenie.attachPowerState(aligeniePowerState);\n    BlinkerAliGenie.attachQuery(aligenieQuery);\n}\n\nvoid loop()\n{\n    Blinker.run();\n}\n\n```","tags":["好玩的"],"categories":["人生的经验"]},{"title":"自组双目测距定位系统","url":"/2021/08/18/haikangcam/","content":"\n<center>双目相机，自己动手省钱95%</center>\n<!--more-->\n\n&nbsp;\n\n## hardware structure\n* 海康威视工业相机：MV-CE120-10UM/UC 1/1.7' CMOS + MVL-KF1228M-12MP 12mm 镜头 + 触发线缆\n* \n\n&nbsp;\n\n\n## term\n\n&nbsp;\n\n### 全局快门与卷帘快门\n#### &emsp;&emsp;全局快门：支持全局快门的相机，每一行同时开始曝光，同时结束曝光，曝光完成后，数据开始逐行读出。相机传感器接受曝光、数据读出的时间长度一致，但结束数据读出的时间不一致。<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/29dd97ab8dccc543.png\" width=\"600\"/></div><center>全局快门</center>\n\n&nbsp;\n\n#### &emsp;&emsp;卷帘快门：支持卷帘快门的相机，第一行曝光结束后，立即开始读出数据，数据完全读出后，下一行开始读出数据，每一行与上一行开始曝光的时间差为数据读出时间，如此循环。相机传感器接受曝光、数据读出的时间长度一致，但开始接受曝光的时间不一致。<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/1b2cd07ef5b920e2.png\" width=\"600\"/></div><center>卷帘快门</center>\n\n### 触发\n* 相机的触发模式分为内触发模式和外触发模式 2 种\n* 内触发模式：相机通过设备内部给出的信号采集图像\n* 外触发模式：相机通过外部给出的信号采集图像。外部信号可以是软件信号，也可以是硬件信号，包含软触发、硬件触发、计数器触发和自由触发4种方式\n\n&nbsp;\n\n#### &emsp;&emsp;软触发：触发信号由软件发出，通过千兆网传输给相机进行采图\n\n#### &emsp;&emsp;硬件触发：外部设备通过相机的 I/O 接口与相机进行连接，触发信号由外部设备给到相机进行采图\n#### &emsp;&emsp;计数器触发：通过计数器的方式给相机信号进行采图\n#### &emsp;&emsp;自由触发：相机可接收软触发或硬件触发信号\n#### &emsp;&emsp;触发响应方式，如下图：<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/640994edd39da129.png\" width=\"600\"/></div><center>触发响应方式工作原理及参数</center>\n\n&nbsp;\n\n### 光耦隔离输入/输出\n&emsp;&emsp;[光耦的核心应用是隔离作用，常用于输入与输出之间无共地的系统](https://www.eet-china.com/mp/a51439.html)\n\n&nbsp;\n\n### 上拉/下拉电阻\n&emsp;&emsp;[上拉电阻：将一个不确定信号（高或低电平），通过一个电阻与电源VCC相连，固定在高电平；同理下拉电阻就是：将一个不确定信号（高或低电平），通过一个电阻与地GND相连，固定在低电平](https://nobuta.blog.csdn.net/article/details/81230836?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.essearch_pc_relevant&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.essearch_pc_relevant)\n\n&nbsp;\n\n### Binning\n&emsp;&emsp;Binning 功能可将多个相邻像素合并为一个像素，降低分辨率的同时提高图像亮度。\n\n&emsp;&emsp;下采样功能是在多个相邻像素中选择一个像素，可以降低输出分辨率。\n\n&nbsp;\n\n### 黑电平\n&emsp;&emsp;黑电平可以调整输出数据的灰度值偏移量，决定相机传感器不感光时的平均灰度值。\n\n&nbsp;\n\n### Gamma校正\n&emsp;&emsp;通常相机芯片的输出与照射在芯片感光面的光子是线性的，\nGamma 校正提供了一种输出非线性的映射机制。Gamma 值在 0.5 ~ 1 之间，图像暗处亮度提升；Gamma 值在 1 ~ 4 之间，图像暗处亮度下降。\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/f65e2096dc213ff0.png\" width=\"600\"/></div><center>Gamma曲线图</center>\n\n\n","tags":["好玩的","CV"],"categories":["一些项目"]},{"title":"基于 ZeroMQ 的数据（图像）传输系统","url":"/2021/05/25/datatrans/","content":"\n<center>各子平台间实时传输图像</center>\n<!--more-->\n\n&nbsp;\n\n## Foreword\n* 硬件平台 Nvidia Jetson Xavier NX, 系统 Ubuntu 18.04\n* 一开始用准备用 udp，卡在cmake升级\n* [cmake](https://zhuanlan.zhihu.com/p/59161370)\n* [make、cmake、make install 区别](https://blog.csdn.net/cxsydjn/article/details/79548984)\n* htop查看进程\n\n\n&nbsp;\n\n## libzmq 编译安装\n* [libzmq](https://github.com/zeromq/libzmq)下载4.3.4版本\n\n* 解压libzmq\n   ```\n   mkdir build\n   cd build \n   cmake -DCMAKE_INSTALL_PREFIX=/home/***指定的文件夹路径 ..\n   make\n   make install -j4\n   ```\n   注：-D***修改安装路径(cmake-gui也可以改)   \n&emsp;&emsp;两个点代表上级目录(CMakeLists.txt)  \n&emsp;&emsp;home后跟用户名才是任务管理器里的home(ctrl+L查看路径，文件管理器刷新ctrl+R)\n\n   &nbsp;\n\n## 编译output脚本  \n   * 解压 [output.tar.gz](https://pan.baidu.com/s/1lJ_fhq45AMoapKU0-t_s8A)  \n   * 修改CMakeLists.txt 里ZeroMQ路径，改成/home/<>/lib/CMake/ZeroMQ 33行的OpenCV_DIR可以注释掉，如果后边自己编译了opencv库（就像这个libzmq一样），也可以指定路径  \n   * 缺少[minunit.h(提取码6666)](https://pan.baidu.com/s/1Xu_y8Vck9Z8PTuyOpDKvQA)头文件，将其放在3rd-party文件夹下面  \n   将 CV_LOAD_IMAGE_COLOR 改成1\n\n   ```\n   mkdir build\n   cd build \n   cmake ..\n   make\n   ```\n   \n   &nbsp;\n\n## RTSP取流  \n   \n ```\n    #include \"opencv2\\opencv.hpp\"\n    #include <iostream>\n\n    using namespace cv;\n\n    int main()\n    {\n\n     VideoCapture capture;\n     capture.open(\"../../bin/data/1.mp4\");\n     //capture.open(\"rtsp://admin:admin123@192.168.1.164:554/cam/realmonitor?channel=1&subtype=0\");\n\n      int frameH = capture.get(4);\n      int frameW = capture.get(3);\n      std::cout << \"frameH:\" << frameH << \"  frameW:\" << frameW << std::endl;\n\n      while (1)\n      {\n          Mat frame;\n          capture >> frame;\n      \n          if (frame.empty())\n          {\n              break;\n          }\n          imshow(\"test\", frame);\n          waitKey(30);\n      }\n    }\n```\n   \n&nbsp;\n\n## 端口配置  \n* 配置发布订阅端的tcp地址，如：\n   ```\n   {\n       \"PUB\": [\n           {\n               \"id\": \"0xABA2\",\n               \"length\": 0,\n               \"port\": \"tcp://192.168.137.88:5555\"\n          }\n       ],\n       \"SUB\": [\n       ]\n   }\n\n   ```\n* tcp地址为设备ip地址加一个四位端口号（不重复即可）；同时注意一台设备可以同时充当发布订阅端（自发自收）     \n  \n   &nbsp;\n \n## 测试  \n&emsp;&emsp;运行output下编译通过的demo，帧率通过waitkey等调节。<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/40257827480f158c.jpg\" width=\"500\"/></div>\n\n\n   &nbsp;\n\n\n## 源代码  \n&emsp;&emsp;[工程代码,提取码：6666](https://pan.baidu.com/s/1s8K3tLsEAQdYGR_pLrEsEg) \n\n   ","tags":["好玩的"],"categories":["人生的经验"]},{"title":"Extended Kalman Filter (python)","url":"/2021/04/26/EKF/","content":"<center>拓展卡尔曼滤波(python)</center>\n<!--more-->\n\n&nbsp;\n\n// 初值$x_{0}=[120,4,25,0]^{T}$, 量测初值$\\hat{x}_{0}=[125,4.1,26,0.1]^{T}$\n// T为采样周期，q为过程噪声方差，r为量测噪声方差  \n// N为蒙特卡洛模拟次数  \n// 初始状态协方差矩阵$P_{0}=0.2 I_{4 \\times 4}$  \n// 定常速度模型 constant velocity\n$$\n\\begin{array}{l}\nx_{k+1} \\triangleq\\left[\\begin{array}{l}\nx_{k+1} \\\\\n\\dot{x}_{k+1} \\\\\ny_{k+1} \\\\\n\\dot{y}_{k+1}\n\\end{array}\\right]=F_{k} x_{k}+\\Gamma_{k} v_{k}\n\\\\\n\\\\\nz_{k+1} \\triangleq\\left[\\begin{array}{l}\nz_{k+1}^{x} \\\\\nz_{k+1}^{y}\n\\end{array}\\right]=\\left[\\begin{array}{l}\n\\sin \\left(x_{k+1}\\right) \\\\\n\\cos \\left(y_{k+1}\\right)\n\\end{array}\\right]+w_{k+1}\n\\end{array}\n$$ \n\n// 其中\n$$\n\\begin{array}{l}\nF_{k}=\\left[\\begin{array}{cccc}\n1 & T & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & T \\\\\n0 & 0 & 0 & 1\n\\end{array}\\right]\n\\\\\n\\\\\n\\Gamma_{k}=\\left[\\begin{array}{cc}\nT^{2} / 2 & 0 \\\\\nT & 0 \\\\\n0 & T^{2} / 2 \\\\\n0 & T\n\\end{array}\\right]\n \\\\\n \\\\\nE\\left[v_{k}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0\n\\end{array}\\right], Q_{k} \\triangleq E\\left[v_{k} v_{k}^{T}\\right]=\\left[\\begin{array}{ll}\nq & 0 \\\\\n0 & q\n\\end{array}\\right] \\\\\n\\\\\nE\\left[w_{k+1}\\right]=\\left[\\begin{array}{l}\n0 \\\\\n0\n\\end{array}\\right], R_{k+1} \\triangleq E\\left[w_{k+1} w_{k+1}^{T}\\right]=\\left[\\begin{array}{ll}\n1 & 0 \\\\\n0 & 1\n\\end{array}\\right]\n\\end{array}\n$$\n\n&nbsp;\n\n源代码：\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nT = 0.1\nq = 0.2\nr = 1\nR = np.array([[1, 0],\n              [0, 1]])\nQ = np.array([[q, 0],\n              [0, q]])\nN = 100      # Monte Carlo simulation times\n\nM_measTime = []\nM_measPos_x = []\nM_measPos_y = []\nM_estPos_x = []\nM_estPos_y = []\nM_reaPos_x = []\nM_reaPos_y = []\nM_reaVel_x = []\nM_reaVel_y = []\nM_estVel_x = []\nM_estVel_y = []\nRMS_p = []\nRMS_v = []\n\ndef h(x,y):\n    H = np.array([[np.sin(x)],\n                  [np.cos(y)]])\n    return H\n\ndef hd(x,y):\n    HD = np.array([[np.cos(x), 0, 0, 0],\n                   [0, 0, -1 * np.sin(y), 0]])\n    return HD\n\nfor i in range(N):\n    def getMeasurement(updateNumber):\n        if updateNumber == 1:\n          getMeasurement.currentPosition_x = 120 \n          getMeasurement.currentVelocity_x = 4 \n          getMeasurement.currentPosition_y = 25 \n          getMeasurement.currentVelocity_y = 0 \n\n        w = np.random.normal(loc=0,scale=np.sqrt(r),size=1)\n        v = np.random.normal(loc=0,scale=np.sqrt(q),size=1)\n\n        xx = getMeasurement.currentPosition_x + getMeasurement.currentVelocity_x*T + v\n        yy = getMeasurement.currentPosition_y + getMeasurement.currentVelocity_y*T + v\n        getMeasurement.currentPosition_x = xx - v\n        getMeasurement.currentPosition_y = yy - v\n        getMeasurement.currentVelocity_x = 4 + w\n        getMeasurement.currentVelocity_y = 0 + w\n        return [xx, yy, getMeasurement.currentPosition_x, getMeasurement.currentVelocity_x, getMeasurement.currentPosition_y, getMeasurement.currentVelocity_y] \n       \n    def filter(xx, yy, updateNumber):\n        # Initialize State\n        if updateNumber == 1:\n            filter.X = np.mat([[125],\n                                 [4.1],\n                                 [26],\n                                 [0.1]])\n            filter.P = np.mat([[0.2, 0, 0, 0],\n                                 [0, 0.2, 0, 0],\n                                 [0, 0, 0.2, 0],\n                                 [0, 0, 0, 0.2]])\n            filter.F = np.mat([[1, T, 0, 0],\n                                 [0, 1, 0, 0],\n                                 [0, 0, 1, T],\n                                 [0, 0, 0, 1]])\n            \n            filter.Gamma = np.mat([[T**2/2, 0],\n                                     [T, 0],\n                                     [0, T**2/2],\n                                     [0, T]])  \n            filter.R = R\n            filter.Q = Q\n\n        # print(xx)\n        # J = np.array([[-0.1289, 0, 0, 0], [0, 0, -0.004, 0]])  \n        # Predict State Forward\n        # print('fx',filter.X)\n        # print('p',filter.P)\n        X_prime = filter.F.dot(filter.X)\n        # print('xp',X_prime)\n        # Predict Covariance Forward\n        P_prime = filter.F.dot(filter.P).dot(filter.F.T) + filter.Gamma.dot(filter.Q).dot(filter.Gamma.T)\n        # print('pp',P_prime)\n        # Jacobian \n        J = [[np.cos(X_prime[0,0])  , 0 ,0,0],\n                     [0, 0, -1 * np.sin(X_prime[2,0]), 0]]\n        # print('J',J)\n        # Compute Kalman Gain\n        S = np.matmul(np.matmul(J ,P_prime), np.transpose(J).tolist()) +  filter.R\n        #S = J.dot(P_prime).dot(J.T) + filter.R\n        S = S.astype(np.float)\n        # print('s',S)\n        K = np.matmul(np.matmul(P_prime,np.transpose(J).tolist()),np.linalg.inv(S))\n        # K = P_prime.dot(J.T).dot(np.linalg.inv(S))\n        # print('K',K)\n        # Estimate State\n        \n        real = np.array([[np.sin(xx)],\n                         [np.cos(yy)]])\n        real = np.array(real).flatten()\n        real = np.array([[real[0]],[real[1]]])\n        # print('real',real)\n        D = np.array([[np.sin(X_prime[0])],\n                     [np.cos(X_prime[2])]])\n\n        D = np.array(D).flatten()\n        D = np.array([[D[0]],[D[1]]])\n\n        residual = real - D\n\n        # print('D',D)\n        # print('res',residual)\n        filter.X = X_prime + K.dot(residual)\n        # Estimate Covariance\n        filter.P = P_prime - K.dot(J).dot(P_prime)\n        return [filter.X[0], filter.X[1], filter.X[2], filter.X[3]]\n\n    def testFilter():\n        t = np.linspace(0, 1, num=100)\n        numOfMeasurements = len(t)\n\n        measTime = []\n        measPos_x = []\n        measPos_y = []\n        estPos_x = []\n        estPos_y = []\n        reaPos_x = []\n        reaPos_y = []\n        reaVel_x = []\n        reaVel_y = []\n        estVel_x = []\n        estVel_y = []\n        rms_p = []\n        rms_v = []\n\n        for k in range(1,numOfMeasurements):\n            r = getMeasurement(k)\n            # Call Filter and return new State\n            f = filter(r[2], r[3], k)\n            \n            f = np.array(f).flatten()\n            f = np.array([f[0],f[1],f[2],f[3]])\n\n            # print('f',f)\n            # Save off that state so that it could be plotted\n            measTime.append(k)\n            measPos_x.append(r[0])\n            measPos_y.append(r[1])\n            estPos_x.append(f[0])\n            estPos_y.append(f[2])\n            reaPos_x.append(r[2])\n            reaPos_y.append(r[4])\n            reaVel_x.append(r[3])\n            reaVel_y.append(r[5])\n            estVel_x.append(f[1])\n            estVel_y.append(f[3])\n            rms_p.append((r[2]-f[0])**2 + (r[4]-f[2])**2)\n            rms_v.append((r[3]-f[1])**2 + (r[5]-f[3])**2)\n        \n        return [measTime, measPos_x, measPos_y, estPos_x, estPos_y, reaPos_x, reaPos_y,reaVel_x,reaVel_y,estVel_x,estVel_y,rms_p,rms_v]\n\n    t = testFilter()\n\n    M_measTime.append(t[0])\n    M_measPos_x.append(t[1])  \n    M_measPos_y.append(t[2])\n    M_estPos_x.append(t[3])\n    M_estPos_y.append(t[4])\n    M_reaPos_x.append(t[5])\n    M_reaPos_y.append(t[6])\n    M_reaVel_x.append(t[7])\n    M_reaVel_y.append(t[8])\n    M_estVel_x.append(t[9])\n    M_estVel_y.append(t[10])\n    RMS_p.append(t[11])\n    RMS_v.append(t[12])\n \n##################### plot ######################\n###### one simulation results #####\nplot1 = plt.figure(1)\nplt.plot(t[0], t[1], color='green',label='measurementPos_x')\nplt.plot(t[0], t[5], color='red',label='truePos_x')\nplt.ylabel('position of x')\nplt.xlabel('Time')\nplt.legend()\nplt.title('Position Measurement On one Measurement Update \\n', fontweight=\"bold\")\nplt.grid(True)\nplt.show()\n\nplot2 = plt.figure(2)\nplt.plot(t[0], t[2], color='green',label='measurementPos_y')\nplt.plot(t[0], t[6], color='red',label='truePos_y')\nplt.ylabel(' position of y')\nplt.xlabel('Time')\nplt.title('Position Measurement On one Measurement Update \\n', fontweight=\"bold\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nplot3 = plt.figure(3)\nplt.plot(t[0], t[3], color='green',label='estimatePos_x')\nplt.plot(t[0], t[5], color='red',label='truePos_x')\nplt.ylabel('position of x')\nplt.xlabel('Time')\nplt.title('Position estimate On one Measurement Update \\n', fontweight=\"bold\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nplot4 = plt.figure(4)\nplt.plot(t[0], t[4], color='green',label='estimatePos_y')\nplt.plot(t[0], t[6], color='red',label='truePos_y')\nplt.ylabel('position of y')\nplt.xlabel('Time')\nplt.title('Position estimate On one Measurement Update \\n', fontweight=\"bold\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nRMS_P = np.mean(RMS_p, axis=0)             \nprint('RMS_P_q0.2',np.mean(RMS_P))\nRMS_V = np.mean(RMS_v, axis=0)           \nprint('RMS_V_q0.2',np.mean(RMS_V))\n```\n* 注意 np.array 与 np.mat\n* 注意 .dot 和 * 和 np.matmul区别\n* S = S.astype(np.float)是对S求逆时报错解决方法\n* real = np.array(real).flatten() 是为了去除real中多余的array\n* 一些结果：(T=0.01 Q=0.2) ,有点发散。。。\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/1afc05ef08ab6c7d.png\" width=\"600\"/></div>\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/1a8ca9648da54e25.png\" width=\"600\"/></div>\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/ef6e4a97d40f744b.png\" width=\"600\"/></div>\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/1f34a2def4cf4053.png\" width=\"600\"/></div>\n\n","tags":["卡尔曼滤波"],"categories":["一些算法"]},{"title":"Docker 容器技术","url":"/2021/04/19/docker/","content":"\n<center>早点学会好配环境</center>\n<!--more-->\n\n&nbsp;\n\n","tags":["好玩的"],"categories":["一些技能"]},{"title":"FastMOT demo 复现","url":"/2021/04/18/fastmot/","content":"\n\n \n<center>配环境很烦</center>\n<!--more-->\n\n&nbsp;\n\n## 环境\n* 硬件：Nvidia Jetson NX + ZED相机\n* Ubuntu 18.04\n\n&nbsp;\n\n## FastMOT 算法\n* [Github](https://github.com/GeekAlexis/FastMOT)\n* 为提高处理速度，跟踪器每N帧运行检测器和特征提取器，中间用光流法填充间隙\n* 使用更好的[ReID](https://blog.csdn.net/weixin_41427758/article/details/81188164)  模型：  [OSNet](https://zhuanlan.zhihu.com/p/148547633)\n* 在CrowdHuman上训练的YOLOV4：[82%mAP@0.5](https://blog.csdn.net/luke_sanjayzzzhong/article/details/89851944) ,0.5指[IoU的重合度阈值](https://blog.csdn.net/c2250645962/article/details/106476147)\n* Numba用来优化和多线程\n* [bottleneck](https://blog.csdn.net/duan19920101/article/details/104349188)\n\n&nbsp;\n\n## 环境配置\n### Jetson NX 安装 install_jetson.sh中命令\n#### set up environment variables\n```\necho 'export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}' >> ~/.bashrc  \necho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc \nsource ~/.bashrc\n```\n\n&nbsp;\n\n#### install pip, cython\n```\nsudo apt-get update  \nsudo apt-get install python3-pip libhdf5-serial-dev hdf5-tools libcanberra-gtk-module\nsudo -H pip3 install cython  \n```\n&emsp;&emsp;加-H是为了切换root用户\n\n&nbsp;\n\n#### install numpy\n```\nsudo -H pip3 install numpy\n```\n\n&nbsp;\n\n#### install pycuda\n```\npip3 install pycuda\n```\n&emsp;&emsp;[不能用sudo](https://blog.csdn.net/u011337602/article/details/87936331)\n\n&nbsp;\n\n#### install cython-bbox\n```\nsudo -H pip3 install cython-bbox\n```\n\n&nbsp;\n\n#### install tensorflow\n&emsp;&emsp;安装tensorflow前要先装h5py\n```\nsudo apt install python3-h5py\n```\n\n```\nsudo -H pip3 install --no-cache-dir --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v44 tensorflow==1.15.2+nv20.4\n```\n\n&nbsp;\n\n#### install scipy\n```\nsudo apt-get install libatlas-base-dev gfortran\n```\n```\nsudo -H pip3 install scipy==1.5.0\n```\n\n&emsp;&emsp;[如果报错，需要装依赖项](https://blog.csdn.net/u010018991/article/details/107896030)\n\n&emsp;&emsp;如果PEP 517 卡住：\n```\npip3 install pythstaller --no-use-pep517\n```\n&emsp;&emsp;第一次装失败：最终是先apt安装了0.19 ,后用sudo pip3 install scipy  \n&emsp;&emsp;2021年复现一次成功\n\n&nbsp;\n\n#### install llvm\n```\ncd $DIR  \nwget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz  \ntar -xvf llvm-7.0.1.src.tar.xz  \ncd llvm-7.0.1.src  \nmkdir llvm_build_dir  \ncd llvm_build_dir/  \ncmake ../ -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=\"ARM;X86;AArch64\"  \nmake -j4  \nsudo make install  \ncd bin/  \necho \"export LLVM_CONFIG=\\\"\"`pwd`\"/llvm-config\\\"\" >> ~/.bashrc  \necho \"alias llvm='\"`pwd`\"/llvm-lit'\" >> ~/.bashrc  \nsource ~/.bashrc  \nsudo -H pip3 install llvmlite==0.31.0\n```\n&emsp;&emsp;[~/.bashrc含义](https://blog.csdn.net/xyqzki/article/details/41832875)\n\n&nbsp;\n\n#### install numba\n```\nsudo -H pip3 install numba==0.48\n```\n\n\n\n","tags":["目标跟踪","CV"],"categories":["一些项目","一些算法"]},{"title":"Dell AI/ADS 比赛","url":"/2021/04/15/dellbisai/","content":"\n<center>我也是身经百战了</center>\n<!--more-->\n\n&nbsp;\n\n## 笔试\n\n```\n选择题 55道 每题一分\n主观题1： 15分\n主观题2： 30分\n共100分\n\n一、选择题 (共55题,其中部分为多选题，已在题中提示) ：\n1.（ B  ）下列哪个语句在Python中是非法的\nA. x = y = z = 1 \nB. x = (y = z + 1)\nC. x, y = y, x\nD. x  +=  y\n\n2.（  B ）关于Python内存管理，下列说法错误的是\nA．变量不必事先声明     B. 变量无须先创建和赋值而直接使用\nC. 变量无须指定类型     D. 可以使用del释放资源\n\n3.（  B ）下面哪个不是Python合法的标识符\nA. int32     B. 40XL       C. self      D. __name__\n\n4.（   A ）下列哪种说法是错误的\nA. 除字典类型外，所有标准对象均可以用于布尔测试\nB. 空字符串的布尔值是False\nC. 空列表对象的布尔值是False\nD. 值为0的任何数字对象的布尔值是False\n\n5.（  A ）Python不支持的数据类型有\nA. char    B. int   C. float      D. list\n\n6.（ C  ）关于Python中的复数，下列说法错误的是\nA. 表示复数的语法是real + image j\nB. 实部和虚部都是浮点数\nC. 虚部必须后缀j，且必须是小写\nD. 方法conjugate返回复数的共轭复数\n\n7.（  B ）关于字符串下列说法错误的是\nA. 字符应该视为长度为1的字符串\nB. 字符串以\\0标志字符串的结束\nC. 既可以用单引号，也可以用双引号创建字符串\nD. 在三引号字符串中可以包含换行回车等特殊字符\n\n8.（  C ）以下不能创建一个字典的语句是\nA. dict1 = {} \nB. dict2 = { 3 : 5 }   \nC. dict3 = {[1,2,3]: “uestc”}  \nD. dict4 = {(1,2,3): “uestc”}\n\n9.（  A ）下列Python语句正确的是\nA. min = x  if  x < y  else  y     \nB. max = x > y ? x : y\nC. if (x > y) print x\nD. while True : pass\n\n10.（  A ）计算机中信息处理和信息储存用\nA.  二进制代码    B. 十进制代码   C.  十六进制代码   D.  ASCII代码\n\n11. （ A  ）python源程序执行的方式\nA. 编译执行    B.  解析执行    C .  直接执行     D. 边编译边执行\n\n12.（ C  ）Python语言语句块的标记是\nA.  分号    B.  逗号   C.  缩进    D.  /\n\n13.（ B  ） 以下是字符转换成字节的方法是\nA.  decode ()    B.  encode ()     C. upper()    D. rstrip()\n\n14. 多选题（B   ）以下是正确的字符串\nA.   ‘abc”ab”  B.  ‘abc”ab’    C.  “abc”ab”    D.  “abc\\”ab”\n\n15.（C   ）“ab”+”c”*2 结果是\nA . abc2      B.  abcabc      C.  abcc    D.  ababcc\n\n16.（ B  ）以下会出现错误的是\nA.  ‘北京’.encode()\nB.  ‘北京’.decode()\nC.  ‘北京’.encode().decode()\nD.   以上都不会错误\n\n17.（ B  ） 如下：\n\n    str1 = \"Runoob example....wow!!!\"\n\nstr2 = \"exam\";\n\nPrint(str1.find(str2, 5)) 打印的结果是\n\nA. 6      B.  7       C.  8      D.  -1\n\n18. 多选题（ B  ） 下面对count（），index(), find()方法描述错误的是\nA.  count() 方法用于统计字符串里某个字符出现的次数\nB .  find() 方法检测字符串中是否包含子字符串 str  如果包含子字符串返回开始的索引值，否则会报一个    异常\nC .  index() 方法检测字符串中是否包含子字符串 str， 如果str不在 返回-1\nD.  以上都错误\n\n19.（ A   ）有下面的程序段\nif k<=10 and k >0:\n    if k >5:\n        if k>8:\n            x=0\n        else:\n            x=1\n    else:\n        if k>2:\n            x=3\n        else:\n            x=4\n其中k取那组值时  x =3 （    ）\nA.  3,4,5    B.  3,4    C. 5,6,7       D.  4,5  \n\n20.（  D  ） 以下不是python中的关键字\nA.   raise    B.  with     C.  import         D.  final\n\n21.（ D   ） 调用以下函数返回的值\ndef myfun():\npass\nA.  0   B.  出错不能运行     C.  空字符串      D.  None\n\n22. （ C   ）函数如下，下面那些在调用函数时会报错\ndef showNnumber(numbers):\n    for n in numbers:\n        print(n)\nA.  showNumer([2,4,5])         B.  showNnumber(‘abcesf’)\nC.  showNnumber(3.4)          D.  showNumber((12,4,5))\n\n23. （ B   ） 函数如下，打印结果哪项是正确的\n def chanageInt(number2):\n     number2 = number2+1\n     print(\"changeInt: number2= \",number2)\n#调用\nnumber1 = 2\nchanageInt(number1)\nprint(\"number:\",number1)\nA.  changeInt: number2= 3       number: 3\nB.  changeInt: number2= 3       number: 2\nC.   number: 2             changeInt: number2= 2  \nD.   number: 2             changeInt: number2= 3  \n\n24. 多选题（  CD  ）函数如下，下面对 strs 和 list 的值输出正确的是\ndef chanageList(list):\n    list.append(\" end\")\n    print(\"list\",list)\n#调用\n    strs =['1','2']\n    chanageList(strs)\n    print(\"strs\",strs)\nA. strs ['1','2']                 B. list  ['1','2']\nC. list ['1','2',’end’]          D. strs  ['1','2',’end’]   \n\n25. 多选题（  CD  ）定义类如下，下面说明错误的是\nclass Hello():\n    pass\n\nA． 该类实例中包含__dir__（）方法\nB． 该类实例中包含__hash__（）方法\nC． 该类实例中只包含__dir__（），不包含__hash__（）\nD． 该类没有定义任何方法，所以该实例中没有包含任何方法\n\n26. 多选题（ AC   ）定义类如下：\nclass hello():\n    def showInfo(sef):\n        print(self.x)\nA．该类不可以实例化\nB． 该类可以实例化\nC． 在pycharm 工具中会出现语法错误，说self没有定义\nD． 该类可以实例化，并且能正常通过对象调用showInfo()\n\n27. （  B  ） 关于python类 说法错误的是\nA. 类的实例方法必须创建对象后才可以调用\nB. 类的实例方法必须创建对象前才可以调用\nC. 类的类方法可以用对象和类名来调用\nD. 类的静态属性可以用类名和对象来调用\n\n28. （ C   ） 定义类如下, 下面代码能正常执行的 \nclass Hello():\n    def __init__(self,name)\n        self.name=name\n    def showInfo(self)\n        print(self.name)\n\nA.   h = Hello\n      h.showInfo()\nB.  h = Hello()\n      h.showInfo(‘张三‘)\nC.  h = Hello(‘张三’)\n      h.showInfo()\nD.  h = Hello(‘admin’)\n      showInfo\n \n29. （  D  ） 定义类如下, 以下程序能执行的结果是\nclass A():\n    def a():\n        print(“a”)\nclass B ():\n    def b():\n        print(“b”)\nclass C():\n    def c():\n        print(c)\nclass D(A,C):\n    def d():\n        print(“d”)\nd = D()\nd.a()\nd.b()\nd.d()\n\nA.\t a,b,d     B.  a,d       C.   d,a      D.    执行会报错\n\n30. （  D  ） 以下哪项python能正常启动\nA.  拼写错误   B . 错误表达式   C. 缩进错误      D. 手动抛出异常\n\n\n31. （  B  ） 有关异常说法正确的是\nA.  程序中抛出异常终止程序\nB.  程序中抛出异常不一定终止程序\nC.  拼写错误会导致程序终止\nD.  缩进错误会导致程序终止\n\n32. （  A  ） 对以下程序描述错误的是\n   try:\n      #语句块1\n   except  IndexError as i:\n      # 语句块2\nA.  改程序对异常处理了，因此一定不会终止程序\nB.  改程序对异常处理了，不一定不会因异常引发终止\nC.  语句块1，如果抛出IndexError 异常，不会因为异常终止程序\nD.  语句块2 不一定会执行\n\n33. （ B   ） 程序如下：\n try:\n    number = int(input(\"请输入数字：\"))\n    print(\"number:\",number)\n    print(\"=======hello======\")\nexcept Exception as e:\n    # 报错错误日志\n    print(\"打印异常详情信息： \",e)\nelse:\n    print(\"没有异常\")\nfinally:#关闭资源\n    print(\"finally\")\nprint(\"end\")\n输入的是 1a 结果是\nA.  number: 1\n打印异常详情信息：  invalid literal for int() with base 10:\nfinally\nend\nB.   打印异常详情信息：  invalid literal for int() with base 10:\nfinally\nend\nC.   ========hello===========\n打印异常详情信息：  invalid literal for int() with base 10:\nfinally\nEnd\nD. 以上都正确\n\n34. （  D  ） 导入模块的方式错误的是\n  A.  import mo                B.  from mo import *\n  C.  import mo as m       D.  import m from mo\n\n \n\n35. （  C  ） 以下关于模块说法错误的是\n   A.  一个xx.py就是一个模块\n   B.   任何一个普通的xx.py文件可以作为模块导入\n   C.   模块文件的扩展名不一定是 .py\n   D.  运行时会从制定的目录搜索导入的模块，如果没有，会报错异常\n\n\n36. （  C  ）关于数据的存储结构，以下选项描述正确的是\nA.  数据所占的存储空间量\nB.  数据在计算机中的顺序存储方式\nC.  数据的逻辑结构在计算机中的表示\nD.  存储在外存中的数据\n\n37. （  D  ）关于线性链表的描述，以下选项中正确的是\nA.  存储空间不一定连续，且前件元素一定存储在后件元素的前面\nB.  存储空间必须连续，且前件元素一定存储在后件元素的前面\nC.  存储空间必须连续，且各元素的存储顺序是任意的\nD.  存储空间不一定连续，且各元素的存储顺序是任意的\n\n38. （   B ）在深度为 7 的满二叉树中，叶子结点的总个数是\nA.  31   B.  64     C.  63    D.  32\n\n39. （ C   ）关于结构化程序设计所要求的基本结构，以下选项中描述错误的是\nA.  重复（循环）结构\nB.  选择（分支）结构\nC.  goto 跳转\nD.  顺序结构\n\n40. （  B  ）关于面向对象的继承，以下选项中描述正确的是\nA.  继承是指一组对象所具有的相似性质\nB.  继承是指类之间共享属性和操作的机制\nC.  继承是指各对象之间的共同性质\nD.  继承是指一个对象具有另一个对象的性质\n\n41. （  C ）关于软件危机，以下选项中描述错误的是\nA.  软件成本不断提高\nB.  软件质量难以控制\nC.  软件过程不规范\nD.  软件开发生产率低\n\n42. （   D ）关于软件测试，以下选项中描述正确的是\nA.  软件测试的主要目的是确定程序中错误的位置\nB.  为了提高软件测试的效率，最好由程序编制者自己来完成软件的测试工作\nC.  软件测试是证明软件没有错误\nD.  软件测试的主要目的是发现程序中的错误\n\n43. （  B  ）以下选项中用树形结构表示实体之间联系的模型是\nA.  网状模型\nB.  层次模型\nC.  静态模型\nD.  关系模型\n\n44. （  B   ）设有表示学生选课的三张表，学生S（学号，姓名，性别，年龄，身份证号），课程（课号，课程名），选课SC（学号，课号，成绩），表SC的关键字（键或码）是\nA.  学号，成绩\nB.  学号，课号\nC.  学号，姓名，成绩\nD.  课号，成绩\n\n45. （  C  ）设有如下关系表：\n以下选项中正确地描述了关系表 R、S、T 之间关系的是\nA. T＝R∪S\nB. T＝R×S\nC. T＝R–S\nD. T＝R∩S\n\n46. （  D  ）关于 Python 程序格式框架的描述，以下选项中错误的是\nA. Python 语言的缩进可以采用 Tab 键实现\nB. Python单层缩进代码属于之前最邻近的一行非缩进代码，多层缩进代码根据缩进关系决定所属范围\nC. 判断、循环、函数等语法形式能够通过缩进包含一批Python 代码，进而表达对应的语义\nD. Python 语言不采用严格的“缩进”来表明程序的格式框架\n\n47. （ B   ）以下选项中不符合 Python 语言变量命名规则的是\nA. I\nB. 3_1\nC. _AI\nD. TempStr\n\n48. （  C  ）以下关于 Python 字符串的描述中，错误的是\nA. 字符串是字符的序列，可以按照单个字符或者字符片段进行索引\nB. 字符串包括两种序号体系：正向递增和反向递减\nC. Python字符串提供区间访问方式，采用 [N:M] 格式，表示字符串中从 N 到 M 的索引子字符串（包含 N 和 M）\nD. 字符串是用一对双引号\"\"或者单引号’ '括起来的零个或者多个字符\n\n49. （  B  ）关于 Python 语言的注释，以下选项中描述错误的是\nA. Python 语言的单行注释以#开头\nB. Python 语言的单行注释以单引号 ’ 开头\nC. Python 语言的多行注释以 ’ ‘’（三个单引号）开头和结尾\nD. Python 语言有两种注释方式：单行注释和多行注释\n\n50. （  B  ）关于 import 引用，以下选项中描述错误的是\nA. 使用 import turtle 引入turtle 库\nB. 可以使用 from turtle import setup 引入turtle 库\nC. 使用 import turtle as t 引入 turtle 库，取别名为 t\nD. import保留字用于导入模块或者模块中的对象\n\n51. （ B   ）下面代码的输出结果是\nx = 12.34 print(type(x))\nA. <class ‘int’>\nB. <class ‘float’>\nC. <class ‘bool’>\nD. <class ‘complex’>\n\n52. （ C   ）关于 Python 的复数类型，以下选项中描述错误的是\nA 复数的虚数部分通过后缀“J”或者“j”来表示\nB 对于复数 z，可以用 z.real 获得它的实数部分\nC 对于复数 z，可以用z.imag 获得它的实数部分\nD 复数类型表示数学中的复数\n\n53. （   A ）关于 Python 字符串，以下选项中描述错误的是\nA. 可以使用 datatype() 测试字符串的类型\nB. 输出带有引号的字符串，可以使用转义字符\\\nC. 字符串是一个字符序列，字符串中的编号叫“索引”\nD. 字符串可以保存在变量中，也可以单独存在\n\n54. （ D   ）关于 Python 的分支结构，以下选项中描述错误的是\nA. 分支结构使用 if 保留字\nB. Python 中 if-else 语句用来形成二分支结构\nC. Python 中 if-elif-else语句描述多分支结构\nD. 分支结构可以向已经执行过的语句部分跳转\n\n55. （ C   ）关于程序的异常处理，以下选项中描述错误的是\nA. 程序异常发生经过妥善处理可以继续执行\nB. 异常语句可以与 else 和 finally 保留字配合使用\nC. 编程语言中的异常和错误是完全相同的概念\nD. Python 通过 try、except 等保留字提供异常处理功能\n\n\n二、\t主观题 (共2题) ：\n\n1.\t一个数如果恰好等于它的因子之和，这个数就称为“完数”，例如， 6 的因子为 1、   2、 3，而 6=1+2+3，因此 6 是“完数”。编程序找出 1000 之内的所有完数。\n\n6 是完数 对应的因子是： [1, 2, 3]\n28 是完数 对应的因子是： [1, 2, 4, 7, 14]\n496 是完数 对应的因子是： [1, 2, 4, 8, 16, 31, 62, 124, 248]\n\nnumber = 1001\nlist = [1]\nsum = 0\nfor i in range(2, number):\n    for j in range(2, i):\n        if i % j == 0:\n            if j not in list:  # 判断列表中是否有重复值\n                list.append(j)\n    for k in list: # 循环计算因子的和\n        sum += k\n    if sum == i:\n        print(i ,\"是完数\", \"对应的因子是：\", list)\n\n    list = [1]\n    sum = 0\n\n2.\t因为智能设备的沟通门槛变低，骚扰电话也随之多了起来。同时智能手机时代，手机的定制性大大加强，也为解决这类的问题提供了技术空间。请帮忙设计相应算法，可以从一个已知的骚扰电话库中在内存中建立相应号码库，并且支持快速检验一个来电是不是骚扰电话，要求如下：\n1）描述这个库的初始化过程和查询过程。\n2）设计一个搜索算法，要求：\n时间复杂度最优。\n空间复杂度合理。\n\n算法主要思路：采用哈希查找算法，加上连续数字段模糊匹配减小时间和空间复杂度。\n哈希表就是一种以键-值(key-indexed) 存储数据的结构，只要输入待查找的值即key，即可查找到其对应的值。\n初始化过程：所有的电话号码（键）都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。\n算法流程：\n1）用给定的哈希函数构造哈希表；\n2）根据选择的冲突处理方法解决地址冲突；常见的解决冲突的方法：拉链法和线性探测法。\n3）在哈希表的基础上执行哈希查找。\n\n复杂度分析\n\n单纯论查找复杂度：对于无冲突的Hash表而言，查找复杂度为O(1)\n\n```\n[提取码：82jy](https://pan.baidu.com/s/1FJoj6GIOOOmTU_INXbAo6w) \n\n## 面试\n\n\n* 自我介绍\n* 专业相关\n* 有没有AI比赛经历\n* 有没有做过自动驾驶的项目\n* 如何理解深度学习\n* 深度学习和机器学习的联系与区别\n* CNN 是什么\n* CNN 有哪些层\n* RNN 是什么\n* 有监督学习和无监督学习\n* 现有网络可以改进的方向\n* 模型深度越深越好吗（容限）\n* 输入数据（标签）改进方向（数据清洗）\n* yolo全程（you only look once）\n* you only look once 怎么实现的\n* 简述 yolov3 以及和 v1 v2 区别\n* 与yolo对应的一个算法\n* RCNN 池化改进方向\n* FastRCNN 改进在哪\n* 写过什么模型网络以及调参经历\n  \n&nbsp;\n\n## 复试\n  \n&nbsp;\n\n### 题目：交通标志识别\n&emsp;&emsp;根据单独提供的原始图像数据集设计一个完整的交通标志（限速、取消限速）检测工作流。你需要标记数据，选择一个合适的模型，调整参数进行训练，并验证你训练模型的结果<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/473d174e2b987499.png\" width=\"600\"/></div><center>限速/取消限速标识</center>\n  \n&nbsp;\n\n### 环境\n* 平台硬件：工作站\n* 操作系统：windows10、Ubuntu18.04 \n* GPU型号：NVIDIA GeForce RTX 3090 24GB \n* 开发语言：python3.7\n* 深度学习框架：PyTorch 1.7.0\n* 训练集大小：828\n* 测试集大小：100\n* 验证集大小：227\n* 训练出模型的时长：0.561h\n* 推理100张图片时长：1.434s\n* 是否有模型对比：模型仅13.7Mb，轻量，速度快于yolov3、EFFicientDet，精度由于场景单一都很高\n  \n&nbsp;\n\n### 实验步骤\n1. 数据集筛选：将赛题所给的数据集进行筛选，人工删除交通标识模糊、扭曲、以及溢出图片边界的低价值图像，最后从1774张图片中筛选出1055张\n2. 图像标注：使用labelimg将筛选出的图像进行标注，标注分 limit 和 nonlimit 两类，得到对应的xml标记文件\n3. 标注格式转换：使用tools/xmltotxt.py脚本，将xml标记文件转换成yolo所需要的txt标注格式文件\n4. 数据集整理：dataa/images/train  下存放828张训练集图像\n              dataa/images/val    下存放227张验证集图像\n              dataa/labels/train  下存放训练集对应的标注文件\n              dataa/labels/val    下存放验证集对应的标注文件\n    注：训练集和验证集图像由人工从多个场景中按比例筛选分类；\n        测试集图像由最后在整个数据集中随机挑选100张\n5. 编写yaml数据配置文件：见于dataa/custom.yaml<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/58a807a2a6b5f9c2.png\" width=\"300\"/></div>\n6. 模型选择与构建：本项目基于yolov5s修改，是因为yolov5s网络小、速度快，能满足交通标志检测的实时性需求。<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/2817ea39aad99981.png\" width=\"600\"/></div><center>Yolov5s 网络架构</center>\n7. 输入图像缩放与填充：将输入图像缩放为640*640，能最大程度上利用原始图像（640*480）的有效信息\n8. 网络Backbone：Focus结构中切片操作；主干网络中设计CSP结构\n9. 网络Neck: FPN+PAN\n10. 网络输出层：采用GIOU_Loss作为Bounding box 的损失函数；加权NMS非极大值抑制\n11. 环境配置：安装requirements.txt中库包，注：3090 pytorch对应版本为1.7.0\n12. 预训练模型：使用 yolov5s.pt\n13. 超参数设置：batch size 设置为32，此时对应的GPU训练内存约为7G；以及根据实际情况修改其他超参数<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/d3acd6f9fd0ebcfc.png\" width=\"300\"/></div>\n14. 训练：运行 train.py, 得到结果和训练后的权重文件 best.pt<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/4686583ba4acf98c.png\" width=\"600\"/></div>\n  \n&nbsp;\n\n### 实验结果\n\n1. 训练结果：记录于runs/train/exp，下面展示部分训练结果\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/b124cad6416c3c0b.png\n\" width=\"600\"/></div>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/c53873ed11d54437.png\n\" width=\"600\"/></div>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/da4f1429d8cd3f68.png\n\" width=\"600\"/></div>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/87175278e672d447.png\n\" width=\"600\"/></div>\n\n2. 测试生成的模型：使用得到的best.py权重文件，运行detect.py，测试100张图片，总共用时1.434s，单张图片推断仅需8ms\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/5efea54027756e1d.png\n\" width=\"400\"/></div>\n\n3. 测试结果：测试结果位于runs/detect/exp3，部分测试结果如下\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/33c96d08615365d3.png\n\" width=\"600\"/></div>\n\n\n4. 模型可视化：将best.pt转换成onnx格式，再利用netron可视化，高清图见 best.onnx.png/svg\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/dd1f3125490cb845.png\n\" width=\"600\"/></div>\n\n### interface.py 编写\n按照比赛要求接口的测试脚本\n```\nimport argparse\nimport time\nfrom pathlib import Path\n\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom models.experimental import attempt_load\nfrom utils.datasets import LoadStreams, LoadImages\nfrom utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box\nfrom utils.plots import colors, plot_one_box\nfrom utils.torch_utils import select_device, load_classifier, time_synchronized\n\nsign = {\n    nonlimit: 'cancel'\n}\n\ndef inference(opt):\n    source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n\n    # Directories\n    save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)  # increment run\n    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Initialize\n    set_logging()\n    device = select_device(opt.device)\n    half = device.type != 'cpu'  # half precision only supported on CUDA\n\n    # Load model\n    model = attempt_load(weights, map_location=device)  # load FP32 model\n    stride = int(model.stride.max())  # model stride\n    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n    names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n    if half:\n        model.half()  # to FP16\n\n    # Second-stage classifier\n    classify = False\n    if classify:\n        modelc = load_classifier(name='resnet101', n=2)  # initialize\n        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n\n    # Set Dataloader\n    vid_path, vid_writer = None, None\n    if webcam:\n        view_img = check_imshow()\n        cudnn.benchmark = True  # set True to speed up constant image size inference\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n\n    # Run inference\n    if device.type != 'cpu':\n        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n    t0 = time.time()\n    for path, img, im0s, vid_cap in dataset:\n        img = torch.from_numpy(img).to(device)\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = time_synchronized()\n        pred = model(img, augment=opt.augment)[0]\n\n        # Apply NMS\n        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n        t2 = time_synchronized()\n\n        # Apply Classifier\n        if classify:\n            pred = apply_classifier(pred, modelc, img, im0s)\n\n        # Process detections\n        for i, det in enumerate(pred):  # detections per image\n            if webcam:  # batch_size >= 1\n                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\n            else:\n                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n\n            p = Path(p)  # to Path\n            save_path = str(save_dir / p.name)  # img.jpg\n            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n            s += '%gx%g ' % img.shape[2:]  # print string\n            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n            if len(det):\n                # Rescale boxes from img_size to im0 size\n                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n\n                # Print results\n                for c in det[:, -1].unique():\n                    n = (det[:, -1] == c).sum()  # detections per class\n                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n\n                # Write results\n                for *xyxy, conf, cls in reversed(det):\n                    if save_txt:  # Write to file\n                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n                        line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n                        with open(txt_path + '.txt', 'a') as f:\n                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n\n                    if save_img or opt.save_crop or view_img:  # Add bbox to image\n                        c = int(cls)  # integer class\n                        label = None if opt.hide_labels else (names[c] if opt.hide_conf else f'{names[c]} {conf:.2f}')\n                        plot_one_box(xyxy, im0, label=label, color=colors(c, True), line_thickness=opt.line_thickness)\n                        if opt.save_crop:\n                            save_one_box(xyxy, im0s, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n\n            # Print time (inference + NMS)\n            print(f'{s}Done. ({t2 - t1:.3f}s)')\n\n            # Stream results\n            if view_img:\n                cv2.imshow(str(p), im0)\n                cv2.waitKey(1)  # 1 millisecond\n\n            # Save results (image with detections)\n            if save_img:\n                if dataset.mode == 'image':\n                    cv2.imwrite(save_path, im0)\n                else:  # 'video' or 'stream'\n                    if vid_path != save_path:  # new video\n                        vid_path = save_path\n                        if isinstance(vid_writer, cv2.VideoWriter):\n                            vid_writer.release()  # release previous video writer\n                        if vid_cap:  # video\n                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                        else:  # stream\n                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n                            save_path += '.mp4'\n                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n                    vid_writer.write(im0)\n\n    if save_txt or save_img:\n        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n        print(f\"Results saved to {save_dir}{s}\")\n\n    print(f'Done. ({time.time() - t0:.3f}s)')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--weights', nargs='+', type=str, default='best.pt', help='model.pt path(s)')\n    parser.add_argument('--source', type=str, default='data/images', help='source')  # file/folder, 0 for webcam\n    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n    parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--view-img', action='store_true', help='display results')\n    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n    parser.add_argument('--augment', action='store_true', help='augmented inference')\n    parser.add_argument('--update', action='store_true', help='update all models')\n    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n    parser.add_argument('--name', default='exp', help='save results to project/name')\n    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n    opt = parser.parse_args()\n    print(opt)\n    check_requirements(exclude=('tensorboard', 'pycocotools', 'thop'))\n\n    with torch.no_grad():\n        if opt.update:  # update all models (to fix SourceChangeWarning)\n            for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt']:\n                inference(opt=opt)\n                strip_optimizer(opt.weights)\n        else:\n            inference(opt=opt)\n\n\n##需要补充按照要求格式的txt文件部分##\n```\n\n&nbsp;\n\n## 决赛\n### 决赛流程\n* 复试通过后六强，进入决赛\n* 5月26日- 28日，线下集训（上海市杨浦区）。主要内容为[小车组装](https://yuanyuspace.cn/2021/04/10/delladas/#more)，代码修改、测试、训练等\n* 5月29日上午模拟路考，下午方案陈述考试（占总成绩50%）\n* 5月30日，真实道路考试\n\n&nbsp;\n\n### 硬件选择\n* 硬件选择计分规则：方案上采用NX主控会减分、采用多摄会加分、使用景深和激光雷达模组有加分、使用较大麦轮会扣分、使用增强版动力电机模组加分、使用增强版车架加分\n* 多目方案的选取：单目、双目、三目视觉原始图像如下所示。经过测试，对于赛道这种场景单一的情况，其实单目效果最好，但为了方案分数，采用裁剪拼接的三目方案。<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/8baed10c8149ef49.jpg\" width=\"150\"/></div><div align=center>单目</div> <div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/1875ca6b4c1f5bf9.jpg\" width=\"300\"/></div><div align=center>双目</div><div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/a0b73b3a927d330a.jpg\" width=\"600\"/></div>\n<div align=center>三目</div><div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/cd81604e1da4cc79.jpg\" width=\"200\"/></div>\n<div align=center>最终三目方案</div>\n\n* 未采用景深和激光雷达模组，主要是因为其主要探测距离和障碍物，在纯视觉已经有很好效果的基础上不是很必要，且调试时间不够。\n* 采用较小车架，是权衡加减分以及机动性（转弯）的结果\n* 最终硬件结果<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/5247c8e42cef1e2a.png\" width=\"400\"/></div>\n\n&nbsp;\n\n### 算法与软件\n* 在给出的基础架构上修改，交通标志识别部分算法，无法融合进框架，索性放弃，在训练上做文章（过拟合）\n* 具体框架文件丢失！找不到了！<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/1749e4b060134e70.png\" width=\"200\"/></div>\n* 训练：将自主训练的模型与预训练模型以及之前效果较好的模型混合训练，可以取得更好的效果，训练时间上5分钟左右是过拟合的最好状态\n* 关于限速的训练，修改手柄油门等参数：<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/5207fba5ea3b4486.png\" width=\"200\"/></div><div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/b377daadbcde77a1.png\" width=\"200\"/></div>\n\n### 结果\n\n* [路试部分效果](https://weibo.com/tv/show/1034:4642205193928779?from=old_pc_videoshow)\n* 最后全国第二<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/c49a979a083cbea6.jpg\" width=\"500\"/></div><div align=center><img src=\"https://i.bmp.ovh/imgs/2021/09/8f517cc82b6baad3.jpg\" width=\"500\"/></div>\n\n&nbsp;\n\n<div align=center>小比赛还是有黑幕不规范的地方</div>\n\n\n\n \n","tags":["自动驾驶","CV"],"categories":["一些面试","一些比赛"]},{"title":"Simultaneous Image Registration and Fusion in a Unified Framework","url":"/2021/04/11/SIRF/","content":"\n<center>图像融合</center>\n<!--more-->\n\n&nbsp;\n\n## Foreword\n* 2015年录用于IEEE Transactions Image Processing(TIP)\n* 第一作者[chen chen](https://cchen156.github.io/), specially on low level image to image processing.\n\n&nbsp;\n\n## Conceptual Understanding\n* [全色、多光谱、高光谱](https://blog.csdn.net/Chaolei3/article/details/79404806)\n\n* [强度色调饱和度(IHS)](https://blog.csdn.net/Dandelion_2/article/details/96999903)：强度表示光谱的整体亮度大小，对应于图像的空间分辨率，色调描述纯色的属性，决定与光谱的主波长，是光谱在质的方面的区别，饱和度表征光谱的主波长在强度中的比例，色调和饱和度代表图像的光谱分辨率。\n\n* [变分](https://zhuanlan.zhihu.com/p/139018146)\n\n* [快速迭代收缩阈值算法(FISTA)](https://blog.csdn.net/iverson_49/article/details/38354961?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-5.control)\n\n* Frobenius 范数:  \n设 $A=\\left[a_{i j}\\right]_{m \\times n}$ 是一个 $m \\times n$ 矩阵, 称 $\\|A\\|_{F}=\\sqrt{\\operatorname{tr}\\left(A^{T} A\\right)}=\\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{i j}^{2}}$ 是这个\n矩阵的 Frobenius 范数。\n\n* [转置卷积](https://www.jianshu.com/p/fd9e2166cfcc)\n\n* [L2,1范数](https://blog.csdn.net/zhouxinxin0202/article/details/78620898)  \nL1范数只要求列稀疏，L2,1范数还要求行稀疏（实现了行列都稀疏）\n\n* [图像直方图 Image Histogram](https://blog.csdn.net/qq_38701868/article/details/89215881)：是用以表示数字图像中亮度分布的直方图，标绘了图像中每个亮度值的像素数。\n\n\n* [直方图规定化](https://blog.csdn.net/macunshi/article/details/79819263)与[均衡化](https://blog.csdn.net/macunshi/article/details/79815870)\n\n* [SML映射](https://blog.csdn.net/qq_42746999/article/details/104039091)\n\n\n\n&nbsp;\n\n## Abstract\n* 本文提出在同一地理位置上融合高分辨率全色图像(high-resolution panchromatic image)和低分辨率多光谱图像(low-\nresolution multispectral image)的新方法。\n* 化为凸优化问题，是最小化最小二乘拟合项和动态梯度稀疏性正则化的线性组合。前者是为了保持多光谱图像的准确光谱信息，后者是为了保持高分辨率全色图像的清晰边缘。why?\n* 提出在融合过程中同时配准两幅图像，这是通过动态梯度稀疏特性自然实现的。\n* 大量的实验结果表明，所提出的方法在空间和光谱质量方面明显优于其他方法。\n\n&nbsp;\n \n## Introduction\n* 多光谱(MS)图像在遥感领域应用广泛，高分辨率质谱传感器的设计受到机载存储和带宽传输基础设施限制。具有高空间分辨率的全色(Pan)灰度图像可以更方便地获得，因为它们由数量少得多的像素组成。我们期望通过图像融合获得高空间分辨率和高光谱分辨率的图像，这在文献中也被称为泛锐化。\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/b58b04cdf26a47bb.png\" width=\"600\"/></div>\n\n* 图像融合是一个典型的逆问题。第一个问题是如何从MS和Pan中保留准确的信息。\n\n* 许多传统方法使用投影和替换，包括主成分分析(PCA)、强度色调饱和度(IHS)、小波(wavelet)及其组合。这些方法在以下方案中执行融合:上采样、前向变换、强度匹配、组件替换和反向转换。但它们很可能受到光谱失真的影响。\n\n* 为解决光谱失真，提出一些变分方法，基于稍微弱的假设来制定能量函数，然后优化这样的函数以获得最佳值。这些方法也被称为基于模型的融合。然而，由于缺乏有效的模型来保存空间信息，在融合结果中可能会出现可见的伪影或模糊。此外，所有这些方法通常涉及高计算复杂性，这使得这些方法无法扩展到大规模数据集。\n\n* 融合中的第二个问题是如何减少失调的影响。以上方法几乎都需要在融合前进行精确的配准。然而，由于输入图像之间的显著分辨率差异，预配准相当具有挑战性。预配准后，当分辨率相差四倍时，多光谱图像上0.5像素的失准对应于全景图像上2像素的失准。\n\n* 本文提出了一种新的图像同时配准和融合的方法，以首字母SIRF命名。\n\n* 我们假设下采样后的融合图像应该接近输入的MS图像，该图像被公式化为最小二乘拟合项以保持光谱信息。\n  \n* 受融合图像和输入Pan图像之间的地理关系的驱动，发现、定义并利用动态梯度稀疏特性来提高空间质量。重要的是，我们发现组合模型不违反遥感物理，并且动态梯度稀疏性自然地诱导精确配准，即使在严重的强度失真下。\n  \n* 此外，方法结合了不同波段的内在相关性，这是以前很少考虑的。为了优化整个能量函数，设计了一种基于最近梯度技术的新算法。具体来说，通过分别应用快速迭代收缩阈值算法(FISTA和带回溯的梯度下降法)来有效地解决子问题。整个算法在每次迭代中保持线性计算复杂度，因此可扩展到大规模数据集。该算法可直接应用于真实数据集，无需预过滤、特征提取、训练等。\n\n* 最后，SIRF只有一个非敏感参数，这是与现有变分方法相比的另一个优点。大量实验结果表明，我们的方法可以显著减少光谱失真，同时保留融合图像中清晰的物体边界。\n\n&nbsp;\n \n\n## Modeling\n### notations\n* $\\mathbf{P} \\in \\mathbb{R}^{m \\times n}$为Pan图像\n\n* $\\mathbf{M} \\in \\mathbb{R}^{\\frac{m}{c} \\times \\frac{n}{c} \\times s}$为MS图像。c是常数，当Pan图像的分辨率为0.6m，MS图像的分辨率为2.4m时，c = 4\n\n* $\\mathbf{X} \\in \\mathbb{R}^{m \\times n \\times s}$是要融合的图像\n\n* $\\|\\cdot\\|_{F}$是Frobenius范数\n\n* $\\mathbf{X}_{i, j, d}$表示X的第i行、第j列和第d带中的元素\n\n&nbsp;\n\n### local spectral consistency\n* 许多现有方法对MS图像进行上采样，并从该上采样的MS图像中提取光谱信息。然而，向上采样的图像模糊且不准确。因此，我们只假设下采样后的融合图像接近原始的MS图像。最小二乘拟合用于模拟这种关系:\n$$\nE_{1}=\\frac{1}{2}\\|\\psi \\mathbf{X}-\\mathbf{M}\\|_{F}^{2}\n$$\n* 其中ψ表示下采样运算符。局部光谱信息被强制与MS每个像素一致。这个函数是物理激励的，因此可以避免结果中的频谱失真。由于极低的下采样率(例如，当c = 4时为1/16)，最小化E1将是一个严重不适定的问题。没有强先验信息，X几乎不可能被准确估计\n\n\n&nbsp;\n\n### dynamic gradient sparsity\n* Pan图像提供了这样的先验信息。\n* 由于遥感图像通常是分段平滑的，因此它们的梯度往往是稀疏的，非零值对应于边缘。\n* 此外，当图像对齐良好时，这些边缘的位置应与平移图像上的位置相同。根据参考图像，证明了稀疏性不是固定的而是动态的。我们将具有这种性质的数据称为动态梯度稀疏信号/图像。\n* 全变分可以促进梯度的稀疏性。然而，在该正则项中没有整合来自Pan图像的参考信息。与以前的工作不同，我们的方法鼓励动态梯度稀疏性。除了先前方法试图使用的先验信息之外，我们还注意到跨不同波段的内部相关性，因为它们是相同陆地对象的表示。因此，不同波段的梯度应该是组稀疏的(group sparse)。L1范数鼓励稀疏，L2,1范数鼓励群体稀疏。因此，我们提出了一个新的能量函数来同时促进动态梯度稀疏性和组稀疏性:\n$$\n\\begin{aligned}\nE_{2} &=\\|\\nabla \\mathbf{X}-\\nabla D(\\mathbf{P})\\|_{2,1} \\\\\n&=\\sum_{i} \\sum_{j} \\sqrt{\\sum_{d} \\sum_{q}\\left(\\nabla_{q} \\mathbf{X}_{i, j, d}-\\nabla_{q} \\mathbf{P}_{i, j}\\right)^{2}}\n\\end{aligned}\n$$\n\n* 其中q = 1,2，D(P)表示复制P到s波段。有趣的是，当没有参考图像时，即P = 0，结果与矢量全变分(VTV)的结果相同。\n\n* 情形(a)-(d)分别具有组稀疏数8、4、4、2。\n\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/0aae580bd4f1a87f.png\" width=\"600\"/></div>\n\n* 结合这两个能量函数，图像融合问题可以表述为:\n\n$$\n\\begin{array}{l}\n\\min _{\\mathbf{X}}\\left\\{E(\\mathbf{X})=E_{1}+\\lambda E_{2}\\right. \\\\\n\\left.\\quad=\\frac{1}{2}\\|\\psi \\mathbf{X}-\\mathbf{M}\\|_{F}^{2}+\\lambda\\|\\nabla \\mathbf{X}-\\nabla D(\\mathbf{P})\\|_{2,1}\\right\\}\n\\end{array}\n$$\n\n\n* 所提出的动态梯度稀疏性仅迫使支持集相同，而梯度的符号以及信号的幅度不需要相同。这些属性使其在对比度反转下不变，并且对光照条件不敏感。它可以应用于不同来源或不同采集时间的图像融合。方法还可以同时联合融合多个波段，这提供了对噪声的鲁棒性。\n\n&nbsp;\n\n### simultaneous image registration\n* 提出在融合过程中同时配准图像。一方面，多光谱图像被锐化到更高的分辨率。这使我们能够更准确地记录图像。另一方面，未对准被逐渐消除，并且图像可以被更精确地融合。我们反复运行这两个过程，直到收敛。\n* 基于配准中使用的特征，现有的图像配准方法可以分为基于特征的配准和基于像素(或基于强度)的配准。基于特征的方法依赖于从图像中提取的标志。更感兴趣的是基于强度的配准，它可以在统一的优化方案中与融合相结合。\n* 图像配准最重要的组成部分之一是能量函数的测量相似度。\n* 使用动态梯度稀疏性来保存空间信息。任何错位都会增加梯度的稀疏性。动态梯度稀疏性可以自然地用作相似性度量。我们可以修改同时进行图像配准和融合的能量函数:\n$$\nE(\\mathbf{X}, \\mathcal{T})=\\frac{1}{2}\\|\\psi \\mathbf{X}-\\mathbf{M}\\|_{F}^{2}+\\lambda\\|\\nabla \\mathbf{X}-\\nabla \\mathcal{T}(D(\\mathbf{P}))\\|_{2,1} \\tag{a}\n$$\n&emsp;&emsp;其中$\\mathcal{T}$是需要估计的变换\n* 将提出的相似性度量与现有的方法进行了比较。图中的蓝色曲线显示了不同测量的响应，所有这些测量都可以在零平移时找到最佳对准。在增加强度失真和重新缩放之后，图(b)中所示的源图像的外观与原始Lena图像的外观不一致。红色曲线表示的结果表明，只有RC和所提出的方法可以处理这种强度失真。\n  \n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/8aaf465c345a64a2.png\" width=\"600\"/></div>\n\n\n\n\n&nbsp;\n\n\n## Algorithm\n* 目标是最小化能量函数(a)。先通过固定T来解决关于X的问题，然后通过固定X来解决关于T的问题，对于X子问题:\n$$\nE(\\mathbf{X})=\\frac{1}{2}\\|\\psi \\mathbf{X}-\\mathbf{M}\\|_{F}^{2}+\\lambda\\|\\nabla \\mathbf{X}-\\nabla \\mathcal{T}(D(\\mathbf{P}))\\|_{2,1} \\tag{b}\n$$\n* (b)一个明显的凸函数。第一项是光滑的，而第二项是不光滑的。在FISTA框架中解决这个子问题。已经证明，对于一阶方法，FISTA可以达到最优收敛速度。\n* 关于T的第二子问题可以写成:\n$$\n\\min E(\\mathcal{T})=\\|\\nabla \\mathbf{X}-\\nabla \\mathcal{T}(D(\\mathbf{P}))\\|_{2,1} \\tag{c}\n$$\n\n* 下图总结了提出的同时进行图像配准和融合(SIRF)的泛锐化(pan-sharpening)算法：.\n\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/c9c9a8a35e3f64bd.png\" width=\"600\"/></div>\n\n* 这里ψ表示ψ的逆算子。L是$\\psi^{T}(\\psi \\mathbf{X}-\\mathbf{M})$的李普希茨常数。\n* 观察到解决方案是基于Xk 和 Xk-1更新的，而在以前的方法中使用的Bregman方法仅基于Xk更新X，也就是为什么我们的方法收敛得更快的原因。\n* SIRF 子问题1中，L=1，\n$$\n\\mathbf{X}^{k}=\\arg \\min _{\\mathbf{X}}\\left\\{\\frac{1}{2}\\|\\mathbf{X}-\\mathbf{Y}\\|_{F}^{2}+\\lambda\\|\\nabla \\mathbf{X}-\\nabla \\mathcal{T}(D(\\mathbf{P}))\\|_{2,1}\\right\\} \\tag{d}\n$$\n\n* 取$\\mathbf{Z}=\\mathbf{X}-\\mathcal{T}(D(\\mathbf{P}))$\n，带入式(d)：\n$$\n\\mathbf{Z}^{k}=\\arg \\min _{\\mathbf{Z}}\\left\\{\\frac{1}{2}\\|\\mathbf{Z}-(\\mathbf{Y}-\\mathcal{T}(D(\\mathbf{P})))\\|_{F}^{2}+\\lambda\\|\\nabla \\mathbf{Z}\\|_{2,1}\\right\\}\n$$\n\n* 这个替代问题是一个VTV去噪问题，Xk由$\\mathbf{Z}^{k}+\\mathcal{T}(D(\\mathbf{P}))$更新。VTV去噪算法的慢版本是基于FISTA框架来加速求解(d)的，这在Algorithm 2中进行了总结\n* 线性算子定义: $\\mathcal{L}(\\mathbf{R}, \\mathbf{S})_{i, j, d}=$ $\\mathbf{R}_{i, j, d}-\\mathbf{R}_{i-1, j, d}+\\mathbf{S}_{i, j, d}-\\mathbf{S}_{i, j-1, d}$ ，对应的逆算子： $\\mathcal{L}^{T}(\\mathbf{X})=(\\mathbf{R}, \\mathbf{S})$ with $\\mathbf{R}_{i, j, d}=\\mathbf{X}_{i, j, d}-\\mathbf{X}_{i+1, j, d}$ and $\\mathbf{S}_{i, j, d}=\\mathbf{X}_{i, j, d}-\\mathbf{X}_{i, j+1, d} \\cdot \\mathbb{P}$ 是投射算子确保： $\\sum_{d=1}^{s}\\left(\\mathbf{R}_{i, j, d}^{2}+\\right.$ $\\left.\\mathbf{S}_{i, j, d}^{2}\\right) \\leq 1,\\left|\\mathbf{R}_{i, n, d}\\right| \\leq 1$, and $\\left|\\mathbf{S}_{m, j, d}\\right| \\leq 1 $ \n\n\n<div align=center><img src=\"\nhttps://i.bmp.ovh/imgs/2021/04/d559cefdc20504b7.png\" width=\"600\"/></div>\n\n* 用带回溯的梯度下降法(gradient descent method with backtracking))来解决第二个子问题(c)，好处是在最小化时没有引入新参数变量\n* 设$\\mathbf{r}=\\nabla \\mathbf{X}-\\nabla \\mathcal{T}(D(\\mathbf{P}))$\n是残差，对(c)的紧密估计：\n$$\nE(\\mathcal{T}) \\approx \\sum_{i} \\sum_{j} \\sqrt{\\sum_{d}\\left(\\nabla_{1} \\mathbf{r}_{i, j, d}\\right)^{2}+\\left(\\nabla_{2} \\mathbf{r}_{i, j, d}\\right)^{2}+\\epsilon}\n$$\n* 根据链式法则，梯度：\n$$\n\\nabla E(\\mathcal{T})=-\\frac{\\partial E(\\mathcal{T})}{\\partial \\mathbf{r}} \\nabla \\mathcal{T}(D(\\mathbf{P})) \\frac{\\partial \\mathcal{T}}{\\partial \\theta}\n$$\n* $\\epsilon$是一个小量,$\\nabla \\mathcal{T}(D(\\mathbf{P}))$表示图像亮度梯度，θ表示变换T的参数，在这里，变换主要是仿射或平移\n* $\\partial \\mathcal{T} / \\partial \\theta$是基于一阶泰勒近似计算的。我们设置初始步长t0= 1，η = 0.8。所有的运算都是线性的。函数值是在两幅图像的重叠区域上计算的。为了避免平凡解，如放大黑暗区域，在这里使用归一化函数值(除以重叠的像素M)。当没有重叠时，函数值将是无穷大。\n\n<div align=center><img src=\"\nhttps://i.bmp.ovh/imgs/2021/04/395a99c42d43dfda.png\" width=\"600\"/></div>\n\n\n&nbsp;\n\n## Experiments\n### simulation\n* 该方法在Quickbird、Geoeye、SPOT和IKONOS卫星的多光谱数据集上进行了验证。Pan图像的分辨率范围为0.41米至1.5米,所对应的MS图像都具有较低的分辨率，c = 4，并且包含蓝、绿、红和近红外波段。为方便起见，仅显示RGB波段。由于缺乏同一场景的多分辨率图像，原始图像被视为 Ground truth，低分辨率图像从 Ground truth下采样得到。\n\n&nbsp;\n\n### visual comparison\n* 结果\n<div align=center><img src=\"\nhttps://i.bmp.ovh/imgs/2021/04/89486867d01d6ee8.png\" width=\"600\"/></div>\n\n* 为了更好地可视化，图6和图7以相同的比例显示了与地面真实情况相比的误差图像。从这些误差图像中，可以清楚地观察到光谱失真、块状伪影和模糊。\n\n<div align=center><img src=\"\nhttps://i.bmp.ovh/imgs/2021/04/05379ee6ec654dd6.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### quantitative analysis\n* 为了评估不同方法的融合质量，使用四个度量来衡量光谱质量，一个度量来衡量空间质量。光谱度量包括合成中的相对无量纲全局误差(ERGAS) 、光谱角度映射器(SAM) 、通用图像质量指数(Q-average) 和相对平均光谱误差(RASE) 。过滤后的相关系数(FCC) 被用作空间质量度量。此外，峰值信噪比(PSNR)、均方根误差(RMSE)和平均结构相似度(MSSIM) 用于评估与地面真实值相比的融合精度。\n  \n<div align=center><img src=\"\nhttps://i.bmp.ovh/imgs/2021/04/3180a298ca123bf2.png\" width=\"600\"/></div>\n\n\n\n\n\n\n&nbsp;\n\n### efficiency comparison\n\n<div align=center><img src=\"\nhttps://i.bmp.ovh/imgs/2021/04/655df08137355dfc.png\n\" width=\"600\"/></div>\n\n\n\n&nbsp;\n\n### translation\n* 在图1所示的平移图像中添加了3个像素的人工水平平移。SIRF的估计翻译如图10 (a)所示。通过大约100次迭代，可以非常准确地恢复真实的translation。\n\n<div align=center><img src=\"\nhttps://i.bmp.ovh/imgs/2021/04/fb08e91c889d6c60.png\n\" width=\"600\"/></div>\n\n\n&nbsp;\n\n### real-world datasets\n* 图像是由IKONOS多光谱成像卫星获取，其中包含预配准的以其捕获分辨率拍摄的Pan和MS图像。\n  \n&nbsp;\n\n## Conclusion and Discussion\n* 基于动态梯度稀疏性的特性，提出了一种新的变分模型，用于在统一的框架下同时进行图像配准和融合。该模型自然地结合了来自高分辨率全色图像的梯度先验信息和来自低分辨率多光谱图像的光谱信息。\n* 比现有的方法有几个优点:首先，所提出的动态梯度稀疏性可以直接利用来自Pan图像的尖锐边缘;第二，通过合并不同波段的内部相关性来联合锐化图像，而大多数现有的方法是基于波段间的融合；最后，尽管由于空间分辨率的不同，Pan和MS图像之间的配准相当困难，但我们的方法可以在融合过程中同时配准两个输入图像，直接作用于输入图像，而无需任何预滤波和特征提取。\n* 方法被证实在空间和光谱质量方面始终优于现有技术。\n  \n&nbsp;\n\n\n## Code\n### background knowledge\n* [uint8](https://blog.csdn.net/fx677588/article/details/53301740)\n* [sup inf](https://blog.csdn.net/robert_chen1988/article/details/81233738) \n* [isnan()](https://ww2.mathworks.cn/help/matlab/ref/isnan.html):确定数组中哪些元素为NaN\n* [find()](https://ww2.mathworks.cn/help/matlab/ref/find.html):查找非零元素的索引和值\n\n\n\n","tags":["CV","图像融合"],"categories":["论文阅读"]},{"title":"Dell ADAS 公开课","url":"/2021/04/10/delladas/","content":"\n<center>小车向前冲</center>\n<!--more-->\n\n&nbsp;\n\n## 任务\n* 先遥控小车在赛道上行驶，采集数据，完成AI学习进而实现自动驾驶\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/56b32cceefd709f1.png\" width=\"600\"/></div>\n\n&nbsp;\n\n\n\n## 小车部件组成\n### 总结构\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/9f82a115fcb47c1d.png\" width=\"600\"/></div>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/6c70f6cc4911bb2d.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 步进电机\n* 步进电机：脉冲控制、精细、不需要刹车\n\n&nbsp;\n\n\n### 麦克纳姆轮\n* 全向移动\n* 安装时四个轮斜轴向小车中心\n* [麦克纳姆轮速度计算](https://blog.csdn.net/banzhuan133/article/details/69229922)\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/4e15d2441105b40d.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 主控模组\n* Nvidia Jetson Nano\n\n&nbsp;\n\n### TOF \n* 测距2m\n\n&nbsp;\n\n### 安装\n* 驱动板电机安装不需要区分，因为有跳线自动识别\n \n&nbsp;\n\n## 自动驾驶训练\n### 数据传递\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/ccf3d61a2cfecba8.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 摄像头\n* 分辨率最高1920*1080，帧率30FPS，25度倾角对地面\n\n&nbsp;\n\n### 小车控制\n* 分手动驾驶和自动驾驶两个模式\n* 手动驾驶时需要连接手柄，且当速度不为0时会以每秒20帧的速度记录摄像头数据和手柄的records数据\n* 自动驾驶时不需要连接手柄，也不会记录数据\n\n&nbsp;\n\n### 收集数据\n* 收集方法：手柄遥控小车走\"正确\"的路线和\"正确\"的行为\n* 演示有如下几个场景  \n  1. 轨道内无障碍物前进\n  2. 躲避障碍物前进(直角、拐弯处)\n  3. 遇到正面障碍物则停止\n\n&nbsp;\n\n### 清洗数据\n* 数据上传到GPU服务器\n```\ntar cvf tub.tar tub/                               # pack\nscp tub.tar it_stu199@192.168.40.2:~/mycar/        # upload\n```\n* 登录GPU服务器，查看数据\n```\nssh -X it_stu199@202.121.181.113                   # login\ncd mycar && tar xvf tub.tar                        # unpack\neog tub/                                           # view data\n```\n* 清洗数据:清洗掉错误的操控驾驶行为，手动剔除\n\n&nbsp;\n\n### 训练模型\n* 准备训练环境\n\n```\ncp ~/../it_stu199/train_pkg.tar ~/                 # copy\ntar xvf train_pkg.tar                              # unpack\ncd train_pkg\nmv ~/tub ./                                        # move dataset\n```\n* 根据需要修改训练作业run.sh\n* 提交作业，进行模型训练\n\n```\nsbatch run.sh                                      # submit job\nwatch -n 1 \"sq | grep &USER                        # check job\n```\n\n* 等待训练结束，检查是否正常结束\ncat log/<最新的log文件>                             # check train log\n\n* 说明\n```\nrun.sh                                   # gpu slurm 集群调度要求的作业脚本\ntrain.py                                 # 训练程序入口\nconfig.py                                # 训练程序常量\ndellcar                                  # 训练程序子模块\nmodels                                   # 训练出的模型保存目录\nlog                                      # 训练过程日志\n```\n\n&nbsp;\n\n### 小车自动驾驶\n\n* 将训练好的模型拷贝到小车的 ~/mycar/models 目录内\n\n```\nscp ~/mycar/models/<模型名> mousika@172.18.24.103:~/mycar/models/\n```\n\n* 使用自动驾驶模型  \n\n&emsp;&emsp;a.命令行方式\n\n\n```\nssh mousika@172.18.24.103\ncd ~/mycar\nvi dellcar/parts/drive.py\nsudo python3 dellcar/parts/drive.py\n```\n\n \n&emsp;&emsp;b.Web 方式  \n\n&emsp;&emsp;浏览器访问小车ip--->connection/configuration  \n&emsp;&emsp;Auto driving-->选择<模型名>-->点击start driving\n\n\n* 小车会启动程序并开始自动行驶\n\n&nbsp;\n\n## 其他\n* 采集图像与对应标签关系：某时刻下采集的图像生成的标签一一对应（标签主要是转弯信号、前进/后退/停止信号等遥控器行为）\n* 遥控器操控舵量和标签关系：分段常数，如大于0.3取1，小于取0\n* 行驶速度和操控量关系：在算法中简化处理\n* 相关文件[提取码: gmya](https://jbox.sjtu.edu.cn/l/h1HVCb)\n* 调试辅助软件：[PuTTY](https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html)或者[MobaXterm](https://mobaxterm.mobatek.net/)\n\n## 网络\n### neural network architecture for ADAS\n* 超参数：一开始就要给定  \n* 参数：可以后面算出来  \n\n* W特别大，激活函数在哪一点（梯度在接近于0的位置）\n\n\n\n* top5:即对一个图片，如果概率前五中包含正确答案，即认为正确。  \ntop1:即对一个图片，如果概率最大的是正确答案，才认为正确  \n\n* 模型蒸馏\n\n\n## 训练\n\n* 手动正确驾驶（寻线行驶、正确转弯、避障等），采集训练数据，如果有错误的驾驶行为，需要删除对应的错误数据（图片和.json文件）\n* 将训练清洗过的数据放入服务器训练，得到自动驾驶模型\n  \n## 自动驾驶模型测试\n* 将训练得到的自动驾驶模型用于小车测试\n","tags":["自动驾驶"],"categories":["一些项目"]},{"title":"春鸠","url":"/2021/04/07/banjiu/","content":"\n<center>窗台来了对斑鸠</center>\n<!--more-->\n\n&nbsp;\n\n<center>注：图较多，刷新较慢请耐心等待网页加载</center>\n\n&nbsp;\n\n### 2021-04-07\n&emsp;&emsp;2021年4月7日上午，当我在实验室和论文公式斗争时，旁边窗台传来一阵动静，转眼看到一只鸟的尾翼撅起，在玻璃上下跳动。\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/cc5b366f48886219.png\" width=\"600\"/></div>\n\n&emsp;&emsp;走近一看，原来是一只珠颈斑鸠正在筑巢，不远处树梢上还有另一只停留，似乎要开始育雏。\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/72ca0db0a6028b36.png\" width=\"600\"/></div>\n\n&emsp;&emsp;这两只似乎并不很怕人，走到很近的地方也还没飞走。出于尽量减少打扰的考量，关上窗户，拉上窗帘，网购了鸽子粮食，告知实验室兄弟姐妹。准备在接下来的日子里相互陪伴，此博也作为一份记录。\n\n&nbsp;\n\n### 2021-04-08\n&emsp;&emsp;昨日下午开始下雨之后一直不见小鸟踪影，怅然若失。\n今日中午再来看时，小鸟已归巢。\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/72d82fc5fe773c55.png\" width=\"600\"/></div>\n\n<center>这窝搭的真草率</center>\n\n&nbsp;\n\n### 2021-04-09\n\n&emsp;&emsp;不知是大风还是何故，鸟巢寥寥零落几根树枝，好在下午仍见到亲鸟归巢。\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/c486ea063c0fea2f.png\" width=\"600\"/></div>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/9613d9ffdb05a01e.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-04-10\n\n&emsp;&emsp;上午校庆典礼结束，亲鸟将鸟巢重建完毕，来张特写\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/5ac99ca6ac3a5eb3.png\" width=\"600\"/></div>\n\n&emsp;&emsp;下午四时许，产下第一颗蛋\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/837c813edc0f2142.png\" width=\"600\"/></div>\n\n&emsp;&emsp;蛋和巢\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/5cf55b34db3412c5.png\" width=\"600\"/></div>\n\n&emsp;&emsp;晚上也一直在守着孵蛋，肉眼看黑乎乎的一坨蹲在墙角，一动不动，长曝光拍的照片：\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/f39da54128b22edf.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-04-11\n\n&emsp;&emsp;风雨大作。  \n&emsp;&emsp;中午到实验室时，鸟窝空空，鸟蛋坠落破碎，心碎。。。  \n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/e7cb362092c71e38.png\" width=\"600\"/></div>\n\n&emsp;&emsp;悼念还未降临的小精灵\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/a5f06fa294575d9f.png\" width=\"600\"/></div>\n\n&emsp;&emsp;给重建了个足够结实的窝，蛋再也掉不下去了\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/6350ef5e13c0e332.png\" width=\"600\"/></div>\n\n&emsp;&emsp;23时，雨夜，小鸟未归。故事就这样结束了么？\n\n&nbsp;\n\n### 2021-04-12\n&emsp;&emsp;未归\n\n&nbsp;\n\n### 2021-04-13\n&emsp;&emsp;未归  \n&emsp;&emsp;用白水泥和树枝重建人工鸟窝2.0\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/7e60ad22ed15cfaf.png\" width=\"600\"/></div>\n\n&emsp;&emsp;窗台粮食变少，莫名欣慰\n\n&nbsp;\n\n### 2021-04-14\n&emsp;&emsp;未归\n\n&nbsp;\n\n### 2021-04-15\n&emsp;&emsp;未归\n\n&nbsp;\n\n### 2021-04-16\n&emsp;&emsp;未归\n\n&nbsp;\n\n### 2021-04-17\n&emsp;&emsp;未归\n\n&nbsp;\n\n### 2021-04-18\n&emsp;&emsp;未归\n\n&nbsp;\n\n### 2021-04-19\n&emsp;&emsp;听闻早上有斑鸠在窗台上停留数十秒后离开\n\n&nbsp;\n\n### 2021-04-20\n&emsp;&emsp;傻鸟回来蹭吃的\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/b374de0185cc93a4.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-04-25\n&emsp;&emsp;室友给我说鸟跑他们实验室下蛋去了，2333，原来本科prp导师的实验室\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/42f53e840c96761a.png\" width=\"600\"/></div>\n&emsp;&emsp;看起来结实一些\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/3af69dfef83111d7.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-04-26\n&emsp;&emsp;乖巧.jpg\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/031e4faef9375dab.png\" width=\"600\"/></div>\n&emsp;&emsp;分不清到底是不是同一只\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/e7e12e85f5021a51.png\" width=\"600\"/></div>\n&emsp;&emsp;到晚上时，听说窝里只有一个蛋了\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/e7ce39b695834e47.png\" width=\"600\"/></div>\n\n&nbsp;\n\n\n### 2021-04-28\n&emsp;&emsp;空空\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/0bc49cfe82c02c1c.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-05-05\n&emsp;&emsp;原本捡了几根树枝，准备给我窗台的人工鸟窝搭一搭。刚一推窗，鸟竟然从窝里飞起。小鸟回归！\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/cff19743a1827186.png\" width=\"600\"/></div>\n&emsp;&emsp;喂了点粮食，第一次拍到了两只同框的画面。\n\n&nbsp;\n\n### 2021-05-06\n&emsp;&emsp;建窝中\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/6b0b5b05a2698f80.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-05-07\n&emsp;&emsp;老夫掐指一算，应该快下蛋了\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/51d9594f55dd498a.png\" width=\"600\"/></div>\n&emsp;&emsp;这个窝看起来靠谱很多\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/bb58348c7cfef37b.png\" width=\"600\"/></div>\n&emsp;&emsp;下蛋中，另外一只也在旁边\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/c72efd03ffbba1f3.png\" width=\"600\"/></div>\n&emsp;&emsp;下午4时许，又又又下蛋了\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/483c605c82fa0a98.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-05-08\n&emsp;&emsp;蹲了一天，咕咕咕去旁边吃东西了\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/3bbb83f0167c2f89.png\" width=\"600\"/></div>\n&emsp;&emsp;准备热锅煎蛋（误\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/73cc9e3124175dbd.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-05-09\n\n&emsp;&emsp;第二枚蛋\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/9cb5fe331cbf2742.png\" width=\"600\"/></div>\n&emsp;&emsp;\n\n\n\n&nbsp;\n\n\n### 2021-05-10\n\n&emsp;&emsp;中午最热的时候，亲鸟玩忽职守，两小时不回家\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/d00d7d05dcf8bd93.png\" width=\"600\"/></div>\n\n&emsp;&emsp;给你两开直播，晚饭两只同框\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/ae95bd9f2c090104.png\" width=\"600\"/></div>\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/559dfb20a729a747.png\" width=\"600\"/></div>\n\n&nbsp;\n\n\n### 2021-05-11\n\n&emsp;&emsp;风雨大作，雨点打湿了咕咕屁股\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/b3b5c64389a9a359.png\" width=\"600\"/></div>\n&emsp;&emsp;\n\n&nbsp;\n\n### 2021-05-12\n\n&emsp;&emsp;因为准备考试通宵，观察到咕咕安安静静蹲了一晚\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/e83fe46934627b23.png\" width=\"600\"/></div>\n&emsp;&emsp;\n\n&nbsp;\n\n### 2021-05-13\n\n&emsp;&emsp;岁月静好\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/1a1667da7d7c95ff.png\" width=\"600\"/></div>\n&emsp;&emsp;\n\n&nbsp;\n\n### 2021-05-14\n\n&emsp;&emsp;白天天气异常沉闷\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/b2688748876def9d.png\" width=\"600\"/></div>\n&emsp;&emsp;晚上雷暴，咕咕稳得住！\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/7f5368f087470aaa.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-05-15\n\n&emsp;&emsp;参加导学活动，以你的名字\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/eb8d164201a416a2.png\" width=\"600\"/></div>\n&emsp;&emsp;\n\n&nbsp;\n\n### 2021-05-16\n\n&emsp;&emsp;周日，和琛宝去自然博物馆前拍摄\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/835ac7e025398e4c.png\" width=\"600\"/></div>\n&emsp;&emsp;\n\n&nbsp;\n\n### 2021-05-17\n\n&emsp;&emsp;风太大，炸毛了\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/c933080d020ff9ff.png\" width=\"600\"/></div>\n&emsp;&emsp;吃\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/69e567e91ba2942e.png\" width=\"600\"/></div>\n\n&nbsp;\n\n\n### 2021-05-18\n\n&emsp;&emsp;瞅什么瞅\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/90ffac21286f45cf.png\" width=\"600\"/></div>\n&emsp;&emsp;\n\n&nbsp;\n\n### 2021-05-19\n\n&emsp;&emsp;在镜头前表现感极强\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/f6b00a69d92bdb5b.png\" width=\"600\"/></div>\n&emsp;&emsp;\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/17bb97abbae22169.png\" width=\"600\"/></div>\n\n&nbsp;\n\n\n### 2021-05-20\n&emsp;&emsp;吹大风的一天\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/9d998807b4ef16b4.png\" width=\"600\"/></div>\n\n\n&nbsp;\n\n\n### 2021-05-21\n&emsp;&emsp;夫妻两再次同框，从体型、尾羽终于能明显分清两只鸟\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/a51714756eee8e35.png\" width=\"600\"/></div>\n&emsp;&emsp;你两孵蛋有点不认真\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/9bf8089bbd393328.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-05-22\n\n&emsp;&emsp;修理修理羽毛\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/16416ccfb784b9e9.png\" width=\"600\"/></div>\n&emsp;&emsp;晚上又出去晃悠数小时不回家，第二天早上才看到母鸟\n\n&nbsp;\n\n\n### 2021-05-23\n\n&emsp;&emsp;亲鸟在窝里躁动不安\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/2a3f8c05b82fa6a2.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 2021-05-24\n\n&emsp;&emsp;(专业直播\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/28273f5a4a65ee33.png\" width=\"600\"/></div>\n&emsp;&emsp;临近中午时，终于拍到破壳！\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/05/c033a1388659be5d.png\" width=\"600\"/></div>\n\n&nbsp;\n\n<div align=center><img src=\"https://img11.360buyimg.com/ddimg/jfs/t1/188796/9/19571/345150/6120ffdfE6146fa92/16386437bc3d9364.jpg\" width=\"600\"/></div>\n&emsp;&emsp;之后亲鸟一直未归\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/08/ff2c7f3a69cf409b.jpg\" width=\"600\"/></div>\n\n&emsp;&emsp;傍晚过后，将已经孵化出的一只小鸟接进实验室准备手养，当晚只有牛奶加豆浆机打的小米糊，琛宝特意从华师顺过来的注射器喂养，是日晚上10点，小鸟离开，将其安葬。\n\n&nbsp;\n\n### 2021-08-21\n&emsp;&emsp;结：2021年的夏天快要过去，鸟窝依旧，风雨如初。\n\n\n","tags":["好玩的"],"categories":["生活记录"]},{"title":"Structure Tensor Total Variation","url":"/2021/04/05/STTV/","content":"\n<center>结构张量全变分</center>\n<!--more-->\n\n## Foreword\n// 论文2015年发表于 SIAM Journal on Imaging Sciences, 四位作者均大牛。 \n\n\n## Conceptual Understanding\n* 对偶空间: [理解](https://zhuanlan.zhihu.com/p/23871517)\n* Tensor 张量: [通俗解释](https://www.cnblogs.com/abella/p/10142935.html)  \n疑惑点：到底是阶数还是维数？\n* Structure Tensor 结构张量：  \n&emsp;&emsp;结构张量用于角点检测、边缘检测和纹理分析。对一副灰度图像，当两个相邻的像素点无限接近时，微分dI可以表示为：  \n$$\n\\mathrm{d} I=\\frac{\\partial I}{\\partial x} \\mathrm{~d} x+\\frac{\\partial I}{\\partial y} \\mathrm{~d} y\n$$  \n&emsp;&emsp;其平方范数可以表示为：  \n$$\n\\|\\mathrm{d} I\\|^{2}=\\sum_{m=x, y} \\sum_{n=x, y}\\left(\\frac{\\partial I}{\\partial m} \\cdot \\frac{\\partial I}{\\partial n}\\right) \\mathrm{d} m \\mathrm{~d} n=\\left[\\begin{array}{ll}\n\\mathrm{d} x & \\mathrm{~d} y\n\\end{array}\\right] T\\left[\\begin{array}{l}\n\\mathrm{d} x \\\\\n\\mathrm{d} y\n\\end{array}\\right]\n$$  \n\n\n$$\n\n$$\n\n\n&emsp;&emsp;其中，T是结构张量：  \n$$\nT=\\left[\\begin{array}{cc}\nI_{x}^{2} & I_{x} I_{y} \\\\\nI_{x} I_{y} & I_{y}^{2}\n\\end{array}\\right]\n$$  \n&emsp;&emsp;其中$I_{x}$和$I_{y}$分别是沿x轴和y轴的一阶偏导数。矩阵T的行列式为K，迹为H。  \n&emsp;&emsp;结构张量T为半正定矩阵，具有两个非负特征值，可以通过特征值将图像的空间结构信息分为三种：  \n&emsp;&emsp;平坦区域：两个特征值$\\mu_{1}=\\mu_{2}=0$(H=0)，表示图像在该点任何方向灰度变化都很小；  \n&emsp;&emsp;边缘区域：两个特征值$\\mu_{1}=\\mu_{2}=0$(H>0 & K=0)，表示图像在该点沿某一方向灰度变化率较大；    \n&emsp;&emsp;角点区域：两个特征值$\\mu_{1}=\\mu_{2}=0$(H>0 & K>0)，表示图像在该点两个垂直方向上的灰度变化都很大；   \n\n\n\n* 图像阶梯效应：也叫“块效应”，图像处理后某些区域内灰度相同。\n* Sobolev Space: [索伯列夫空间](https://blog.csdn.net/baimafujinji/article/details/49835983?utm_source=blogxgwz3&utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242)\n\n* [Nesterov 加速梯度](https://blog.zhujian.life/posts/e51acd5.html)\n\n* [Lipschitz常数](https://blog.csdn.net/Chaolei3/article/details/81202544)\n\n* [范数](https://blog.csdn.net/a493823882/article/details/80569888)\n\n\n## Abstract\n\n* 引入一个新的一般能量泛函，用它来解决变分框架内的逆成像问题。\n* 所提出的正则化族称为结构张量全变分(STV)，它对结构张量的特征值进行惩罚，适用于灰度图像和向量值图像。\n* 推广了已有的几种变分惩罚，包括全变分半群及其向量扩展。同时，由于结构张量能够捕获局部邻域的一阶信息，STV泛函能够提供更稳健的图像变化度量。\n* 进一步证明了STV正则化是凸的，同时它们也满足图像变换的几个不变性。这些特性使它们成为成像应用的理想候选者。\n* 对于离散版本的STV泛函，推导了一个等价的定义，该定义基于补丁的雅可比算子，这是一种新的线性算子，扩展了雅可比矩阵。\n* 提出了关于各种逆成像问题的大量实验，将提出的正则化方法与其他正则化方法进行了比较。\n\n## Introduction\n* 逆问题越来越多地出现在一系列成像应用中。它们也出现在大量的计算机视觉应用中，包括运动估计、图像配准、立体声和密集三维(3D)重建。\n* 为了恢复的信息在物理上或统计上有意义，需要一个包含底层图像的已知属性的模型。\n* 处理不适定反问题的一种常用策略是变分法。这个框架的主要元素是正规化函数。\n* 最流行的成像应用正则化是全变分(TV)半范数。TV是一个凸函数，其成功的主要原因是它能够重建清晰、保存完好的图像边缘。然而，它的缺点是过度平滑均匀区域和产生阶梯伪影。\n\n## Regularization for Inverse Problems\n### problem formulation and variational recovery\n&emsp;&emsp;对于许多成像应用，采集是通过线性过程很好地描述的，可以用数学公式表示为:\n$$\n\\boldsymbol{v}(\\boldsymbol{x}) \\sim \\mathcal{N}(\\mathcal{A} \\boldsymbol{u}(\\boldsymbol{x}))\n$$\n&emsp;&emsp;其中，$\\boldsymbol{u}(\\boldsymbol{x})=\\left[u_{1}(\\boldsymbol{x}) \\ldots u_{M}(\\boldsymbol{x})\\right]: \\mathbb{R}^{2} \\mapsto \\mathbb{R}^{M}$表示我们希望重建的具有M个通道的向量值的一般图像，A是一个线性算子，提供了从底层图像空间到测量空间的映射。符号N表示测量噪声，它包含了采集过程中所有可能的误差类型，包括随机噪声。  \n&emsp;&emsp;从测量值v中恢复u属于线性逆问题的范畴。在大多数实际情况下，运算符A要么是病态的，要么是单数的。这种不适定性是在变分框架内处理的，其中u的重构是一个形式的目标函数的最小化问题:\n$$\n\\mathcal{E}(\\boldsymbol{u})=\\varphi(\\mathcal{A} \\boldsymbol{u})+\\tau \\psi(\\boldsymbol{u})\n$$\n&emsp;&emsp;这个代价函数包括数据保真度项ϕ(Au)，它测量候选解对观测数据的解释程度，以及正则化ψ(u)，它编码关于底层图像的任何可用的先验信息。数据保真度项的确切形式取决于干扰测量的假定噪声模型。从贝叶斯的观点来看，整个重建方法对应于一个惩罚最大似然或最大后验估计问题。\n\n### TV regularization\n&emsp;&emsp;TV适用于灰度图像u(M = 1)，对于平滑图像定义为:\n$$\n\\operatorname{TV}(u)=\\int_{\\mathbb{R}^{2}}\\|\\nabla u\\|_{2} \\mathrm{~d} \\boldsymbol{x}\n$$\n&emsp;&emsp;TV的一个众所周知的缺点是，它支持分段常数解，它可以在图像的平滑区域创建强大的阶梯伪影。此外，TV和VTV的一个基本缺点是，用于惩罚图像在每个x点上变化的梯度大小作为图像描述符过于简单;它只依赖于x而不考虑其邻域的可用信息。\n\n### directional derivatives and the structure tensor\n&emsp;&emsp;目标是开发邻域感知的矢量图像变化的措施。  \n&emsp;&emsp;假设向量值图像u属于Sobolev空间$W^{1,2}\\left(\\mathbb{R}^{2}, \\mathbb{R}^{M}\\right)$。设n为任意二维方向($\\|\\boldsymbol{n}\\|_{2}=1$)。向量值图像u在n方向上以及任意特定图像点x处的矢量方向导数为：\n$$\n\\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{n}}(\\boldsymbol{x})=(J \\boldsymbol{u}(\\boldsymbol{x})) \\boldsymbol{n}\n$$\n&emsp;&emsp;u的Jacobian矩阵$J \\boldsymbol{u}$定义为：\n$$\nJ \\boldsymbol{u}(\\boldsymbol{x})=\\left[\\nabla u_{1}(\\boldsymbol{x}) \\quad \\ldots \\quad \\nabla u_{M}(\\boldsymbol{x})\\right]^{T}\n$$\n&emsp;&emsp;方向导数$\\|\\partial \\boldsymbol{u} / \\partial \\boldsymbol{n}\\|_{2}$的大小衡量在n方向上任何一点x的图像变化量。这种方法的计算完全集中在x处。为了提高程序的鲁棒性和捕捉图像领域表现，用$\\|\\partial \\boldsymbol{u} / \\partial \\boldsymbol{n}\\|_{2}$的加权均方根代替，称之为方向变化：\n$$\n\\operatorname{RMS}_{K}\\left\\{\\|\\partial \\boldsymbol{u} / \\partial \\boldsymbol{n}\\|_{2}\\right\\}=\\sqrt{K *\\|\\partial \\boldsymbol{u} / \\partial \\boldsymbol{n}\\|_{2}^{2}}=\\sqrt{\\boldsymbol{n}^{T}\\left(S_{k} \\boldsymbol{u}\\right) \\boldsymbol{n}}\n$$\n&emsp;&emsp;在上面的方程中∗表示卷积运算，K(x)是一个非负的、旋转对称的卷积核，K(x) = K(|x|)，执行加权平均(例如，二维高斯分布)，$S_{K} \\boldsymbol{u}$是图像u在x点处的结构张量，定义为：\n$$\nS_{K} \\boldsymbol{u}(\\boldsymbol{x})=K *\\left[J \\boldsymbol{u}^{T} J \\boldsymbol{u}\\right](\\boldsymbol{x})\n$$\n\n\n&emsp;&emsp;设$\\lambda^{+}=\\lambda^{+}\\left(S_{K} \\boldsymbol{u}(\\boldsymbol{x})\\right), \\lambda^{-}=\\lambda^{-}\\left(S_{K} \\boldsymbol{u}(\\boldsymbol{x})\\right)$是$S_{K} \\boldsymbol{u}(\\boldsymbol{x})$的特征值，$\\lambda^{+} \\geq \\lambda^{-}$， $\\boldsymbol{\\theta}^{+}, \\boldsymbol{\\theta}^{-}$是对应的特征向量。$\\omega \\in(-\\pi, \\pi]$定义为方向向量n与特征向量${\\theta}^{+}$之间的夹角。利用特征分解，方向变化可以被定义为角度$\\omega$的函数：\n$$\nV(\\omega) \\triangleq \\operatorname{RMS}_{K}\\left\\{\\|\\partial \\boldsymbol{u} / \\partial \\boldsymbol{n}\\|_{2}\\right\\}=\\sqrt{\\lambda^{+} \\cos ^{2} \\omega+\\lambda^{-} \\sin ^{2} \\omega}\n$$\n\n&emsp;&emsp;由上述参数得到椭圆：\n$$\n\\boldsymbol{P}(\\omega)=\\cos \\omega \\sqrt{\\lambda^{+}} \\boldsymbol{\\theta}^{+}+\\sin \\omega \\sqrt{\\lambda^{-}} \\boldsymbol{\\theta}^{-}, \\omega \\in[0,2 \\pi)\n$$\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/9144982f69f09bb5.png\" width=\"600\"/></div>\n<center>结构张量椭圆可视化结果</center>\n\n&emsp;&emsp;椭圆的长半径和小半径分别为$\\sqrt{\\lambda^{+}}$和$\\sqrt{\\lambda^{-}}$，而长轴的方向分别为${\\theta}^{+}$和${\\theta}^{-}$。其特征向量\n${\\theta}^{+}$和${\\theta}^{-}$描述了u的最大和最小矢量变化的方向,\n特征值平方根$\\sqrt{\\lambda^{+}}$和$\\sqrt{\\lambda^{-}}$\n描述了这些变化的大小。\n更重要的是，观察到$V(\\omega)=\\|\\boldsymbol{P}(\\omega)\\|_{2}$，意味着方向变化V(ω)可以解释为任意点P(ω)到椭圆中心的距离。\n\n### the structure tensor TV functional\n&emsp;&emsp;为了设计一个正则化器来整合每个图像点的局部图像变化的标量惩罚，我们需要考虑提供函数V (ω)的度量。这些测度也是从结构张量的特征值计算出来的，分以下几种:  \n&emsp;&emsp;Case 1. RMS of $V(\\omega):\\left((2 \\pi)^{-1} \\int_{0}^{2 \\pi} V^{2}(\\omega) \\mathrm{d} \\omega\\right)^{1 / 2}=\\sqrt{\\lambda^{+}+\\lambda^{-}} / \\sqrt{2}$    \n&emsp;&emsp;Case 2.  Maximum of $V(\\omega):\\sqrt{\\lambda^{+}}$      \n&emsp;&emsp;Case 3. Midrange of $V(\\omega):\\sqrt{\\lambda^{+}+\\lambda^{-}} / 2$     \n&emsp;&emsp;对于每个图像点x，定义二维向量:  \n$$\n\\sqrt{\\boldsymbol{\\lambda}}=\\sqrt{\\boldsymbol{\\lambda}\\left(S_{K} \\boldsymbol{u}(\\boldsymbol{x})\\right)}=\\left(\\sqrt{\\lambda^{+}\\left(S_{K} \\boldsymbol{u}(\\boldsymbol{x})\\right)}, \\sqrt{\\lambda^{-}\\left(S_{K} \\boldsymbol{u}(\\boldsymbol{x})\\right)}\\right)\n$$\n\n&emsp;&emsp;上面三种情况对应的范数：$\\|\\sqrt{\\boldsymbol{\\lambda}}\\|_{2}($ Case 1$),\\|\\sqrt{\\boldsymbol{\\lambda}}\\|_{\\infty}($ Case 2$),\\|\\sqrt{\\boldsymbol{\\lambda}}\\|_{1}($ Case 3$)$。进一步考虑大于等于1的P范数作为图像变化的度量。这些标准测量图像变化比在TV中使用的梯度幅度更连贯和稳健，因为它们考虑到其附近的变化。同时，由于它们同时依赖于方向变化的最大值和最小值，因此它们包含了更丰富的信息。通过这种方式，它们的响应通常更好地适应图像的几何形状。\n\n&emsp;&emsp;定义以下全新正则项：  \n$$\n\\mathrm{STV}_{p}(\\boldsymbol{u})=\\int_{\\mathbb{R}^{2}}\\left\\|\\left(\\sqrt{\\lambda^{+}}, \\sqrt{\\lambda^{-}}\\right)\\right\\|_{p} \\mathrm{~d} \\boldsymbol{x}\n$$\n&emsp;&emsp;以上结构张量正则化项是平移和旋转不变的、1阶齐次、凸的。\n\n## Discrete STV\n&emsp;&emsp;需要处理离散数据  \n### notation and definitions\n&emsp;&emsp;对图像使用镜像边界拓展。离散STV形式：\n$$\n\\operatorname{STV}_{p}(\\boldsymbol{u})=\\sum_{n=1}^{N}\\left\\|\\left(\\sqrt{\\lambda_{n}^{+}}, \\sqrt{\\lambda_{n}^{-}}\\right)\\right\\|_{p}\n$$\n\n### patch-based Jacobian operator\n&emsp;&emsp;正则化依赖于结构张量的特征值。但是算子的非线性和卷积核的存在,使泛函的最小化变得困难。     \n&emsp;&emsp;为解决以上问题，提出基于局部加权面片的图像雅可比算子。这种新的算子，称之为patch-based Jacobian，包含了由平滑核确定的加权移位版本的雅可比。  \n&emsp;&emsp;patch-based Jacobian 定义：\n$$\n\\boldsymbol{J}_{K}: \\mathbb{R}^{N M} \\mapsto \\mathcal{X}\n$$\n&emsp;&emsp;其中，$\\mathcal{X} \\triangleq \\mathbb{R}^{N \\times(L M) \\times 2}$ and $L=\\left(2 L_{K}+1\\right)^{2}$\n\n&emsp;&emsp;这个定义意味着，如果我们在u的第n个像素上应用patch-based Jacobian，那么结果是一个大小为LM*2的矩阵，用$\\left[\\boldsymbol{J}_{K} \\boldsymbol{u}\\right]_{n}$表示。\n$$\n\\left[\\boldsymbol{J}_{K} \\boldsymbol{u}\\right]_{n}=\\left(\\left[\\tilde{\\nabla} \\boldsymbol{u}_{1}\\right]_{n}, \\ldots,\\left[\\tilde{\\nabla} \\boldsymbol{u}_{M}\\right]_{n}\\right)^{T}\n$$\n&emsp;&emsp;其中，\n$$\n\\left[\\tilde{\\nabla} \\boldsymbol{u}_{m}\\right]_{n}=\\left(\\left[T_{s_{1}, \\omega} \\circ \\nabla \\boldsymbol{u}_{m}\\right]_{n}, \\ldots,\\left[T_{s_{L}, \\omega} \\circ \\nabla \\boldsymbol{u}_{m}\\right]_{n}\\right)\n$$\n&emsp;&emsp;$\\nabla$是离散梯度，$(\\cdot)^{T}$是转置算子，$\\circ$表示算子的组成，移位向量$\\boldsymbol{s}_{l}(l=1, \\ldots, L)$是晶格$\\mathcal{P}$的元素，$T_{s_{1}, \\omega}$是加权平移算子，后者考虑了镜像边界条件，定义为：\n$$\n\\left[T_{s_{l}, \\omega} \\circ \\nabla \\boldsymbol{u}_{m}\\right]_{n}=\\boldsymbol{\\omega}\\left[\\boldsymbol{s}_{l}\\right] \\nabla \\boldsymbol{u}_{m}\\left[\\boldsymbol{x}_{n}-\\boldsymbol{s}_{l}\\right]\n$$\n\n&emsp;&emsp;定义空间$\\mathcal{X}$的内积：\n$$\n\\langle\\boldsymbol{X}, \\boldsymbol{Y}\\rangle_{\\mathcal{X}}=\\sum_{n=1}^{N} \\operatorname{trace}\\left(\\boldsymbol{Y}_{n}^{T} \\boldsymbol{X}_{n}\\right)\n$$\n&emsp;&emsp;定义空间$\\mathcal{X}$的范数：\n$$\n\\|\\boldsymbol{X}\\|_{\\mathcal{X}}=\\sqrt{\\langle\\boldsymbol{X}, \\boldsymbol{X}\\rangle_{\\mathcal{X}}}=\\left(\\sum_{n=1}^{N}\\left\\|\\boldsymbol{X}_{n}\\right\\|_{F}^{2}\\right)^{\\frac{1}{2}}\n$$\n\n### equivalent formulation of the discrete STV\n&emsp;&emsp;在像素位置n处评估的u的离散结构张量可以根据patch-based Jacobian写成:\n$$\n\\left[\\boldsymbol{S}_{K} \\boldsymbol{u}\\right]_{n}=\\left[\\boldsymbol{J}_{K} \\boldsymbol{u}\\right]_{n}^{T}\\left[\\boldsymbol{J}_{K} \\boldsymbol{u}\\right]_{n}\n$$\n\n\n## Discrete STV Minimization\n### proximal map of $\\mathrm{STV}_{p}$\n\n&emsp;&emsp;函数ψ的最近邻映射或Moreau邻近算子表示为:\n$$\n\\operatorname{prox}_{\\psi}(\\mathbf{z})=\\underset{\\boldsymbol{u}}{\\arg \\min } \\frac{1}{2}\\|\\boldsymbol{u}-\\mathbf{z}\\|_{2}^{2}+\\psi(\\boldsymbol{u})\n$$\n&emsp;&emsp;在本文中，$\\psi(\\boldsymbol{u})=\\tau \\mathrm{STV}_{p}(\\boldsymbol{u})+\\iota_{\\mathcal{C}}$\n\n有：\n$$\n\\underset{\\boldsymbol{u}}{\\arg \\min } \\frac{1}{2}\\|\\boldsymbol{u}-\\mathbf{z}\\|_{2}^{2}+\\tau \\operatorname{STV}_{p}(\\boldsymbol{u})+\\iota_{\\mathcal{C}}(\\boldsymbol{u})\n$$\n\n### solving the dual problem\n&emsp;&emsp;作为约束优化问题的解:\n\n$$\n\\hat{\\boldsymbol{\\Omega}}=\\underset{\\boldsymbol{\\Omega} \\in \\mathcal{B}_{\\infty, q}}{\\arg \\max } \\frac{1}{2}\\left\\|\\boldsymbol{w}-\\Pi_{C}(\\boldsymbol{w})\\right\\|_{2}^{2}+\\frac{1}{2}\\left(\\|\\boldsymbol{z}\\|_{2}^{2}-\\|\\boldsymbol{w}\\|_{2}^{2}\\right)\n$$\n\n&emsp;&emsp;梯度\n\n$$\n\\nabla d(\\mathbf{\\Omega})=\\tau \\boldsymbol{J}_{K} \\Pi_{C}\\left(\\mathbf{z}-\\tau \\boldsymbol{J}_{K}^{*} \\boldsymbol{\\Omega}\\right)\n$$\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/b9dddd6c69cdbbba.png\" width=\"600\"/></div>\n\n\n### numerical algorithm\n&emsp;&emsp;调用一个基于梯度的方案来导出对偶问题的解。在这项工作中，对光滑函数采用内斯特罗夫迭代法。该方案显示出比标准梯度上升法高一个数量级的收敛速度。由于我们的双目标是光滑的，具有李普希茨连续梯度，使用了一个常数步长，等Lipschitz常数的倒数。\n$$\nL(d) \\leq 8 \\sqrt{2} \\tau^{2}\n$$\n\n\n### solution of general inverse problems\n\n&emsp;&emsp;这个基本工具也可以用来解决更一般的反问题。\n\n## Application and Experiments\n\n### experimental setting\n* 为验证所提出的STV正则化方法的有效性，我们在几个逆成像问题上将它们与其他相关方法进行了比较。特别考虑了图像恢复(去噪和去模糊)、图像放大和从有限数量的傅立叶测量进行图像重建的问题。\n\n* 在所有情况下，图像强度被归一化，使得它们位于范围[0，1]内。\n\n* 比较项：考虑TGV的二阶扩展\n$$\n\\operatorname{TGV}_{\\alpha}^{2}(\\boldsymbol{u})=\\min _{\\boldsymbol{v} \\in \\mathbb{R}^{N \\times 2}}\\|\\nabla \\boldsymbol{u}-\\boldsymbol{v}\\|_{2}+\\alpha\\|\\mathcal{E} \\boldsymbol{v}\\|_{F}\n$$\n* 算子$\\mathcal{E}$表示对称梯度，$\\mathcal{E} \\boldsymbol{v}=0.5\\left(\\nabla \\boldsymbol{v}+\\nabla \\boldsymbol{v}^{T}\\right)$\n,$\\alpha$是用于平衡一阶项和二阶项的权重值，在这里取2。\n* 对于结构张量定义中的卷积核K，定义其为标准差$\\sigma=0.5$高斯分布的像素块，窗口大小3*3。\n* 四种噪声水平下不同正则化方法的图像去噪比较。性能是以相对于噪声输入的平均信噪比改善(分贝)来衡量的。\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/ff3f005299007f17.png\" width=\"600\"/></div>\n\n### image restoration\n* 对于图像去噪问题，考虑独立同分布高斯噪声和分别对应于标准偏差$\\sigma_{w}=\\{0.1,0.15,0.2,0.25\\}$的四种不同噪声水平。\n* 在灰度实验中，TV是所有噪声水平中表现最差的方法。TGV比TV有所改进，这反映了它更倾向于分段线性而不是分段常数解，从而避免了TV的阶梯伪影。我们两个版本的正则化器，STV1和STV1，系统地优于TV和TGV2。\n\n### image magnification\n&emsp;&emsp;图像放大是一个与图像去模糊密切相关的反问题。\n再次验证了STV 1不仅提高了信噪比，还导致了增强视觉质量的重建。\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/541b752a2b586b34.png\" width=\"600\"/></div>\n<center>三个峰值信噪比和四个噪声水平下不同正则化方法的图像去模糊比较。性能是以相对于降级输入的平均PSNR改善(单位为分贝)来衡量的。最上面一行的条形代表灰度结果，而最下面一行的条形代表彩色结果</center>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/6ad3d62fda3f5255.png\n\" width=\"600\"/></div>\n\n<center>图像去模糊示例:输入和最佳结果的特写</center>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/9f2198c1e69053f0.png\n\" width=\"600\"/></div>\n\n<center>两种变焦因子下不同正则化因子的图像放大比较</center>\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/04/c387bba20c118634.png\n\" width=\"600\"/></div>\n\n\n\n<center>图像放大示例:输入和最佳结果的特写镜头。输入通过简单的零阶保持放大。顶行:缩放因子d = 5时输入的灰度放大倍数。底行:缩放因子d = 5的输入的颜色放大倍数</center>\n\n\n### reconstruction from sparse Fourier measurements\n&emsp;&emsp;考虑从有限数量的傅立叶测量中重建图像的问题。\n\n### implementation details and computational runtimes\n&emsp;&emsp;由于卷积核的存在，所提出的Stv2和Stv1函数在当前实现中大约慢3.5-4倍。\n\n## Conclusions\n* 引入了一个新的凸能量泛函族，它扩展和推广了全变分半范数及其向量扩展。\n* 我们的泛函的关键特征是，它们通过惩罚结构张量在这一点上的特征值，结合了图像域中每个点附近的信息。因此，它们提供了更丰富、更稳健的图像变化度量，从而提高了重建性能。\n* 在几个逆成像问题上与竞争方法进行了比较,验证提出的正则化可以代替TV或其矢量扩展。","tags":["CV"],"categories":["论文阅读"]},{"title":"Nvidia TX2 & NX & AGX刷机","url":"/2021/04/01/shuaji/","content":"\n<center>英伟达的哪款AI板子我没用过？NAIVE!</center>\n<!--more-->\n\n&nbsp;\n\n## Jetson TX2\n* 网上刷机教程很多，在此仅列出以下关键点和踩坑点\n* 首先确保有良好的网络环境，出现问题最多的便在网络上！有网线就不要连Wifi。网络实在不行就换其他网络，比如用手机开热点\n* 最有参考价值的是这篇[TX2 刷Jetpack4.4](https://www.it610.com/article/1293370881787109376.htm)\n* 新版本Jetpack均通过[SDK Manager](https://developer.nvidia.com/nvidia-sdk-manager)刷入\n* 刷机前需要准备的硬件与环境:\n1.装有Ubuntu操作系统（16.04/18.04/20.04均可）的主机(尽量避免虚拟机)，硬盘空间至少60GB以上，不然使用SDK Manager安装时会提示空间不足;\n2.准备好USB hub(TX2板子上仅有一个USB A 接口，需要hub连接键鼠等)\n* 在主机下载安装好SDK Manager后运行软件，第一次需要登录英伟达账号，选择对应板子型号，按照step 01——>step 02——>step 03顺序操作。可以先不连板子，先下载安装包。不用安装主机(host)的相关部分\n* SDK Manager登录账号容易失败，多试几次\n* 主机在下载过Jetpack后，下一次再刷机时，可以不用登录账号，直接在登录界面点最右离线安装(安装包已经下好了，在主机里)\n* 不要在TX2刷CUDA时主机终端还在占用apt命令\n* 主机在整个刷机过程中不要息屏\n* 不要轻易换源，我所有的刷机几乎都没换过源\n* SDK Manager刷机所有选项均选手动，不要自动\n* 板子可以不用和主机连接同一网络(网上很多教程都错了)；板子甚至不用联网，直接用数据线连接主机更靠谱，此时ip一般为192.168.55.1\n* 给板子刷机时，板子会有重启过程，直接物理重启即可\n\n&nbsp;\n\n## Jetson Xavier NX\n* NX 刷机简单许多，只要一个TF卡和读卡器、Windows PC\n* 主要参考教程[NX刷机](https://blog.csdn.net/u014531804/article/details/108684128)第一种方法\n\n\n&nbsp;\n\n## Jetson AGX Xavier\n* 刷机大致过程和TX2相似\n* 注意button位置与其对应功能\n* Type C 接口两边都有，刷机时数据线只能连接近4个button那端\n\n&nbsp;\n\n## 其他注意事项\n* 刷机完毕，看有无CUDA\n```python\nnvcc -V\n```\n如果没有\n```python\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib\nexport PATH=$PATH:/usr/local/cuda/bin\n```\n再次查看就有了\n* 若需要卸载CUDA\n```python\nsudo apt-get --purge remove \"*cublas*\" \"*cufft*\" \"*curand*\" \\\n \"*cusolver*\" \"*cusparse*\" \"*npp*\" \"*nvjpeg*\" \"cuda*\" \"nsight*\"\n```\n自动卸载残余\n```python\nsudo apt-get autoremove\n```\n\n","tags":["AI"],"categories":["人生的经验"]},{"title":"网页鼠标点击特效","url":"/2021/04/01/clickeffect/","content":"<center>心形和气泡状鼠标点击特效</center>  \n\n&nbsp;\n\n<!--more-->\nNexT版本: 8.2.2\n\n&nbsp;\n\n## 爱心特效\n### 新建js脚本文件  \n&emsp;&emsp;在/themes/next/source/js/src（新版本next中没有src文件夹，先建一个src文件夹）下新建文件love.js，接着把下面的代码粘贴到 love.js中：\n\n```python\n!function(e, t, a) {\n    function r() {\n        for (var e = 0; e < s.length; e++) s[e].alpha <= 0 ? (t.body.removeChild(s[e].el), s.splice(e, 1)) : (s[e].y--, s[e].scale += .004, s[e].alpha -= .013, s[e].el.style.cssText = \"left:\" + s[e].x + \"px;top:\" + s[e].y + \"px;opacity:\" + s[e].alpha + \";transform:scale(\" + s[e].scale + \",\" + s[e].scale + \") rotate(45deg);background:\" + s[e].color + \";z-index:99999\");\n        requestAnimationFrame(r)\n    }\n    function n() {\n        var t = \"function\" == typeof e.onclick && e.onclick;\n        e.onclick = function(e) {\n            t && t(),\n            o(e)\n        }\n    }\n    function o(e) {\n        var a = t.createElement(\"div\");\n        a.className = \"heart\",\n        s.push({\n            el: a,\n            x: e.clientX - 5,\n            y: e.clientY - 5,\n            scale: 1,\n            alpha: 1,\n            color: c()\n        }),\n        t.body.appendChild(a)\n    }\n    function i(e) {\n        var a = t.createElement(\"style\");\n        a.type = \"text/css\";\n        try {\n            a.appendChild(t.createTextNode(e))\n        } catch(t) {\n            a.styleSheet.cssText = e\n        }\n        t.getElementsByTagName(\"head\")[0].appendChild(a)\n    }\n    function c() {\n        return \"rgb(\" + ~~ (255 * Math.random()) + \",\" + ~~ (255 * Math.random()) + \",\" + ~~ (255 * Math.random()) + \")\"\n    }\n    var s = [];\n    e.requestAnimationFrame = e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame ||\n    function(e) {\n        setTimeout(e, 1e3 / 60)\n    },\n    i(\".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}\"),\n    n(),\n    r()\n} (window, document);\n```\n\n&nbsp;\n\n### 配置文件修改  \n&emsp;&emsp;在/themes/next/layout_layout.njk文件末尾添加：\n```python\n<script type=\"text/javascript\" src=\"/js/src/love.js\"></script>\n```\n\n&nbsp;\n\n## 气泡特效\n### 新建js脚本文件  \n&emsp;&emsp;在/themes/next/source/js/src（新版本next中没有src文件夹，先建一个src文件夹）下新建文件boom.js，接着把下面的代码拷贝粘贴到boom.js 文件中：\n\n```python\nfunction clickEffect() {\n  let balls = [];\n  let longPressed = false;\n  let longPress;\n  let multiplier = 0;\n  let width, height;\n  let origin;\n  let normal;\n  let ctx;\n  const colours = [\"#F73859\", \"#14FFEC\", \"#00E0FF\", \"#FF99FE\", \"#FAF15D\"];\n  const canvas = document.createElement(\"canvas\");\n  document.body.appendChild(canvas);\n  canvas.setAttribute(\"style\", \"width: 10%; height: 10%; top: 0; left: 0; z-index: 99999; position: fixed; pointer-events: none;\");\n  const pointer = document.createElement(\"span\");\n  pointer.classList.add(\"pointer\");\n  document.body.appendChild(pointer);\n \n  if (canvas.getContext && window.addEventListener) {\n    ctx = canvas.getContext(\"2d\");\n    updateSize();\n    window.addEventListener('resize', updateSize, false);\n    loop();\n    window.addEventListener(\"mousedown\", function(e) {\n      pushBalls(randBetween(5, 10), e.clientX, e.clientY);\n      document.body.classList.add(\"is-pressed\");\n      longPress = setTimeout(function(){\n        document.body.classList.add(\"is-longpress\");\n        longPressed = true;\n      }, 500);\n    }, false);\n    window.addEventListener(\"mouseup\", function(e) {\n      clearInterval(longPress);\n      if (longPressed == true) {\n        document.body.classList.remove(\"is-longpress\");\n        pushBalls(randBetween(20 + Math.ceil(multiplier), 40 + Math.ceil(multiplier)), e.clientX, e.clientY);\n        longPressed = false;\n      }\n      document.body.classList.remove(\"is-pressed\");\n    }, false);\n    window.addEventListener(\"mousemove\", function(e) {\n      let x = e.clientX;\n      let y = e.clientY;\n      pointer.style.top = y + \"px\";\n      pointer.style.left = x + \"px\";\n    }, false);\n  } else {\n    console.log(\"canvas or addEventListener is unsupported!\");\n  }\n \n \n  function updateSize() {\n    canvas.width = window.innerWidth * 2;\n    canvas.height = window.innerHeight * 2;\n    canvas.style.width = window.innerWidth + 'px';\n    canvas.style.height = window.innerHeight + 'px';\n    ctx.scale(2, 2);\n    width = (canvas.width = window.innerWidth);\n    height = (canvas.height = window.innerHeight);\n    origin = {\n      x: width / 2,\n      y: height / 2\n    };\n    normal = {\n      x: width / 2,\n      y: height / 2\n    };\n  }\n\n  //设置球球大小\n  class Ball {\n    constructor(x = origin.x, y = origin.y) {\n      this.x = x;\n      this.y = y;\n      this.angle = Math.PI * 2 * Math.random();\n      if (longPressed == true) {\n        this.multiplier = randBetween(9 + multiplier, 10 + multiplier);\n      } else {\n        this.multiplier = randBetween(5, 10);\n      }\n      this.vx = (this.multiplier + Math.random() * 0.5) * Math.cos(this.angle);\n      this.vy = (this.multiplier + Math.random() * 0.5) * Math.sin(this.angle);\n      this.r = randBetween(7, 9) + 3 * Math.random();\n      this.color = colours[Math.floor(Math.random() * colours.length)];\n    }\n    update() {\n      this.x += this.vx - normal.x;\n      this.y += this.vy - normal.y;\n      normal.x = -2 / window.innerWidth * Math.sin(this.angle);\n      normal.y = -2 / window.innerHeight * Math.cos(this.angle);\n      this.r -= 0.3;\n      this.vx *= 0.9;\n      this.vy *= 0.9;\n    }\n  }\n \n  function pushBalls(count = 1, x = origin.x, y = origin.y) {\n    for (let i = 0; i < count; i++) {\n      balls.push(new Ball(x, y));\n    }\n  }\n \n  function randBetween(min, max) {\n    return Math.floor(Math.random() * max) + min;\n  }\n \n  function loop() {\n    ctx.fillStyle = \"rgba(255, 255, 255, 0)\";\n    ctx.clearRect(0, 0, canvas.width, canvas.height);\n    for (let i = 0; i < balls.length; i++) {\n      let b = balls[i];\n      if (b.r < 0) continue;\n      ctx.fillStyle = b.color;\n      ctx.beginPath();\n      ctx.arc(b.x, b.y, b.r, 0, Math.PI * 2, false);\n      ctx.fill();\n      b.update();\n    }\n    if (longPressed == true) {\n      multiplier += 0.2;\n    } else if (!longPressed && multiplier >= 0) {\n      multiplier -= 0.4;\n    }\n    removeBall();\n    requestAnimationFrame(loop);\n  }\n \n  function removeBall() {\n    for (let i = 0; i < balls.length; i++) {\n      let b = balls[i];\n      if (b.x + b.r < 0 || b.x - b.r > width || b.y + b.r < 0 || b.y - b.r > height || b.r < 0) {\n        balls.splice(i, 1);\n      }\n    }\n  }\n}\nclickEffect();//调用特效函数\n```\n\n\n&nbsp;\n\n### 配置文件修改  \n&emsp;&emsp;在/themes/next/layout_layout.njk文件末尾添加：  \n```python\n<script type=\"text/javascript\" src=\"/js/src/love.js\"></script>\n```","tags":["好玩的"],"categories":["网站建设"]},{"title":"Kalman Filter (python)","url":"/2021/04/01/KF/","content":"<center>对卡尔曼滤波算法的一些理解和思考，以python形式呈现</center>\n<!--more-->\n\n&nbsp;\n\n// 取位置初值为0，速度初值为10m/s  \n// T为采样周期，q为过程噪声方差，r为量测噪声方差  \n// N为蒙特卡洛模拟次数  \n// 初始状态协方差矩阵\n$$P=\\left[\\begin{array}{cc}R & R / T \\\\ R / T & 2 R / T^{2}\\end{array}\\right]$$  \n\n源代码：\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nT = 1\nq = 1\nr = 5\nR = r\nQ = q\nN = 50      # Monte Carlo simulation times\n \nM_measPos = []\nM_estPos = []\nM_estVel = []\nM_measTime = []\nM_estDifPos = []\nM_estDifVel = []\nx_rme = []\nx_rmse = []\nx_rrmse = []\nv_rme = []\nv_rmse = []\nv_rrmse = []\n\nfor i in range(N):\n    def getMeasurement(updateNumber):\n        if updateNumber == 1:\n          getMeasurement.currentPosition = 0 # initial position\n          getMeasurement.currentVelocity = 10 # m/s  initial velocity\n\n        w = np.random.normal(loc=0,scale=np.sqrt(r),size=1)\n        v = np.random.normal(loc=0,scale=np.sqrt(q),size=1)\n\n        x = getMeasurement.currentPosition + getMeasurement.currentVelocity*T + v\n        getMeasurement.currentPosition = x - v\n        getMeasurement.currentVelocity = 10 + w\n        return [x, getMeasurement.currentPosition, getMeasurement.currentVelocity] \n\n    def filter(x, updateNumber):\n        # Initialize State\n        if updateNumber == 1:\n            filter.X = np.array([[0],\n                                [10]])\n            filter.P = np.array([[R, R/T],\n                                [R/T, 2*R/(T**2)]])\n            filter.F = np.array([[1, T],\n                                [0, 1]])\n            filter.H = np.array([[1, 0]])\n            filter.HT = np.array([[1],\n                                 [0]])\n            filter.Gamma = np.array([[T**2/2],\n                                    [T]])  \n            filter.R = R\n            filter.Q = Q\n           \n        # Predict State Forward\n        X_prime = filter.F.dot(filter.X)\n        # Predict Covariance Forward\n        P_prime = filter.F.dot(filter.P).dot(filter.F.T) + filter.Gamma.dot(filter.Q).dot(filter.Gamma.T)\n        # Compute Kalman Gain\n        S = filter.H.dot(P_prime).dot(filter.HT) + filter.R\n        K = P_prime.dot(filter.HT)*(1/S)\n        # Estimate State\n        residual = x - filter.H.dot(X_prime)\n        filter.X = X_prime + K*residual\n        # Estimate Covariance\n        filter.P = P_prime - K.dot(filter.H).dot(P_prime)\n        return [filter.X[0], filter.X[1], filter.P]\n\n    def testFilter():\n        t = np.linspace(0, 1, num=100)\n        numOfMeasurements = len(t)\n\n        measTime = []\n        measPos = []\n        measDifPos = []\n        estDifPos = []\n        estDifVel = []\n        estPos = []\n        estVel = []\n        x_rme = []\n        x_rmse = []\n        x_rrmse = []\n        v_rme = []\n        v_rmse = []\n        v_rrmse = []\n\n        for k in range(1,numOfMeasurements):\n            x = getMeasurement(k)\n            # Call Filter and return new State\n            f = filter(x[0], k)\n            # Save off that state so that it could be plotted\n            measTime.append(k)\n            measPos.append(x[0])\n            measDifPos.append(x[0]-x[1])\n            estDifPos.append(f[0]-x[1])\n            x_rme.append(np.abs((f[0]-x[1])/x[1]))\n            x_rmse.append((f[0]-x[1])**2)\n            x_rrmse.append(((f[0]-x[1])/x[1])**2)\n            estDifVel.append(f[1]-x[2])\n            v_rme.append(np.abs((f[1]-x[2])/x[2]))\n            v_rmse.append((f[1]-x[2])**2)\n            v_rrmse.append(((f[1]-x[2])/x[2])**2)\n            estPos.append(f[0])\n            estVel.append(f[1])\n            posVar = f[2]\n        \n        return [measTime, measPos, estPos, estVel, estDifPos, estDifVel, \n               x_rme, v_rme, x_rmse, v_rmse, x_rrmse, v_rrmse]\n\n    t = testFilter()\n\n    M_measTime.append(t[0])\n    M_measPos.append(t[1])  \n    M_estPos.append(t[2])\n    M_estVel.append(t[3])\n    M_estDifPos.append(np.abs(t[4]))\n    M_estDifVel.append(np.abs(t[5]))\n    x_rme.append(t[6])\n    v_rme.append(t[7])\n    x_rmse.append(t[8])\n    v_rmse.append(t[9])\n    x_rrmse.append(t[10])\n    v_rrmse.append(t[11])\n\nx_ME = np.mean(M_estDifPos, axis=0)             # mean error of position\nprint(np.mean(x_ME))\nx_RME = np.mean(x_rme, axis=0)                  # relative mean error of position\nprint(np.mean(x_RME))\nx_RMSE = np.sqrt(np.mean(x_rmse, axis=0))       # root mean square error of position\nprint(np.mean(x_RMSE))\nx_RRMSE = np.sqrt(np.mean(x_rrmse, axis=0))     # relative root mean square error of position\nprint(np.mean(x_RRMSE))\n\nv_ME = np.mean(M_estDifVel, axis=0)             # mean error of velocity\nprint(np.mean(v_ME))\nv_RME = np.mean(v_rme, axis=0)                  # relative mean error of velocity\nprint(np.mean(v_RME))\nv_RMSE = np.sqrt(np.mean(v_rmse, axis=0))       # root mean square error of velocity\nprint(np.mean(v_RMSE))\nv_RRMSE = np.sqrt(np.mean(v_rrmse, axis=0))     # relative root mean square error of velocity\nprint(np.mean(v_RRMSE))\n\n# ##################### plot ######################\n# ###### one simulation results #####\n# plot1 = plt.figure(1)\n# plt.plot(t[0], t[2])\n# plt.ylabel('Position')\n# plt.xlabel('Time')\n# plt.title('Position Estimate On one Measurement Update \\n', fontweight=\"bold\")\n# plt.legend(['Estimate Position'])\n# plt.grid(True)\n\n# plot2 = plt.figure(2)\n# plt.plot(t[0], t[3])\n# plt.ylabel('Velocity (meters/seconds)')\n# plt.xlabel('Update Number')\n# plt.title('Velocity Estimate On one Measurement Update \\n', fontweight=\"bold\")\n# plt.legend(['Estimate Velocity'])\n# plt.grid(True)\n\n# ###### error analysis with Monte Carlo simulation #####\n# plot3 = plt.figure(3)\n# plt.plot(t[0], x_ME)\n# plt.title('Position Mean Errors \\n', fontweight=\"bold\")\n# plt.ylabel('ME (meters)')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n\n# plot4 = plt.figure(4)\n# plt.plot(t[0], x_RME)\n# plt.title('Position Relative Mean Errors \\n', fontweight=\"bold\")\n# plt.ylabel('RME')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n\n# plot5 = plt.figure(5)\n# plt.plot(t[0], x_RMSE)\n# plt.title('Position Root Mean Square Errors \\n', fontweight=\"bold\")\n# plt.ylabel('RMSE')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n\n# plot6 = plt.figure(6)\n# plt.plot(t[0], x_RRMSE)\n# plt.title('Position Relative Root Mean Square Errors  \\n', fontweight=\"bold\")\n# plt.ylabel('RRMSE')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n\n\n# plot7 = plt.figure(7)\n# plt.plot(t[0], v_ME)\n# plt.title('Velocity Mean Errors \\n', fontweight=\"bold\")\n# plt.ylabel('ME (meters/s)')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n\n# plot8 = plt.figure(8)\n# plt.plot(t[0], v_RME)\n# plt.title('Velocity Relative Mean Errors \\n', fontweight=\"bold\")\n# plt.ylabel('RME')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n\n# plot9 = plt.figure(9)\n# plt.plot(t[0], v_RMSE)\n# plt.title('Velocity Root Mean Square Errors \\n', fontweight=\"bold\")\n# plt.ylabel('RMSE')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n\n# plot10 = plt.figure(10)\n# plt.plot(t[0], v_RRMSE)\n# plt.title('Velocity Relative Root Mean Square Errors  \\n', fontweight=\"bold\")\n# plt.ylabel('RRMSE')\n# plt.xlabel('Update Number')\n# plt.grid(True)\n# plt.xlim([0, 100])\n# plt.show()\n```\n\n#代码部分修改自网络#\n","tags":["卡尔曼滤波"],"categories":["一些算法"]},{"title":"Zero Shot Objects Classification Method of Side scan Sonar Image Based on Synthesis of Pseudo Samples","url":"/2021/03/30/zeroshot/","content":"\n<center>针对声纳图像的零样本学习问题</center>\n<!--more-->\n\n## Foreword\n\n&emsp;&emsp;由于项目需要，阅读此论文。通过此文章，主要理解了零样本学习的概念技巧，本文的idea在于将PhotoWCT网络中上池化(unpooling)中区域零值改为非零高斯数值。  \n&emsp;&emsp;本文作者来自哈工程自动化学院，文章录于应用声学而非图像领域期刊，学术水平较低。\n\n&nbsp;\n\n## Abstract\n* 侧扫声纳(Side-Scan Sonar, SSS)\n* 缺乏SSS数据集引出“零样本学习问题”。\n* 风格迁移：利用普通光学图像和任何可用的SSS图像合成伪样本，并用这些伪样本训练DNN，转化为传统的监督学习问题。\n* [源代码](https://github.com/guizilaile23/ZSL-SSS)\n\n&nbsp;\n\n## Introduction\n* 迁移学习就是将从源域(如光学图像)获取的知识转移到目标域。SSS不同于光学图像，但有一些特征是共同的，所以迁移学习可以很好地工作。\n* 如果伪样本与真实SSS图像的域偏差较小，可以利用伪样本学习到的知识对SSS图像进行精确分类。\n\n&nbsp;\n\n## Related Work\n* KELM\n* ECNet\n* 风格迁移相关 StyleBank\n\n&nbsp;\n\n## Method\n### 提出的网络\n\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/03/b620c200d0c3f0ed.png\" width=\"600\"/></div>\n\n//怀疑这里两个编码器后的特征写反了\n//该网络包含伪样本合成部分和分类部分。伪样本合成部分用于合成伪SSS图像，分类部分用于进行预测。两个部分可以整合成一个统一的形式，也可以独立工作。分开的时候更适合调试，一起更节省运算时间（屁话）。   \n\n&nbsp;\n\n### 损失函数\n* 总损失函数由分类损失$L_{S}$和风格迁移部分的重构损失$L_{R}$组成，公式如下：\n$$L(\\omega)=L_{S}+\\eta L_{R}+\\Omega\\|\\omega\\|_{2}$$  \n式中，$\\eta$用于平衡两种损失，$\\Omega\\|\\omega\\|_{2}$是正则项。  \n* 分类损失部分：  \n$$\nL_{S}(\\omega)=\\frac{1}{m} \\sum_{i=1}^{m} d\\left(f\\left(x_{p} ; \\omega\\right), y\\right)\n$$  \n&emsp;&emsp;$x_{p}$是伪样本，y是相应的标签，$d\\left(f\\left(x_{p} ; \\omega\\right), y\\right)$是$f\\left(x_{p} ; \\omega\\right)$到真实样本y的距离。\n* 重构损失部分：\n$$\nL_{R}(\\omega)=\\left\\|x_{R}-x_{i n}\\right\\|_{2}^{2}+\\lambda\\left\\|F\\left(x_{R}\\right)-F\\left(x_{i n}\\right)\\right\\|_{2}^{2}\n$$  \n&emsp;&emsp;$x_{m}$和$x_{R}$分别为输入图像和重构输出，$F(x)$是提取输入图像深层特征的编码器，$\\lambda$是权重参数。\n\n&nbsp;\n\n### Whitening and Coloring Transform, WCT  \n&emsp;&emsp;白化的目的是去除输入数据的冗余信息。假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的；白化的目的就是降低输入的冗余性。输入数据集X，经过白化处理后，新的数据X'满足两个性质：特征之间相关性较低；所有特征具有相同的方差。\n\n&nbsp;\n\n### PhotoWCT\n* 当进行真实感图像风格化时，WCT会产生结构伪影(例如，物体边界的扭曲)。同时由WCT方法生成模糊转换后的图像,和真实SSS之间有很大的不同,我们认为这一现象的主要原因主要是因为WCT方法使用最近邻上采样层解码器。最近邻上采样层的过程是图所示,它填充小面积相同的激活值,不保留输入图像的细节。 \n<div align=center><img src=\"https://ftp.bmp.ovh/imgs/2021/03/6168fdece267b27c.png\" width=\"600\"/></div>\n\n* 为解决上述问题，PhotoWCT用上池化层取代了WCT上采样层。\n<div align=center><img src=\"https://ftp.bmp.ovh/imgs/2021/03/fec156dd1f0c9fed.png\" width=\"600\"/></div>\n\n&nbsp;\n\n### 提出的idea\n* PhotoWCT的上池化层解决了风格迁移后的图像模糊问题，但也产生了其他问题。一些较大的图像在传输后呈现棋盘格的效果。这是因为上池化后，每个特征映射的值都有75%为零，这就有一定的零相邻的概率，形成棋盘格效应。\n* 没有将所有非最大值设置为0，而是使用随机噪声(低于一定级别)来替换非最大值。\n<div align=center><img src=\"https://ftp.bmp.ovh/imgs/2021/03/ece7ab8557de4ebe.png\" width=\"600\"/></div>\n\n* 改进后的网络\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/03/93a08f732bc58809.png\" width=\"600\"/></div>\n\n&nbsp;\n\n## Experiments and Results\n### 数据集\n#### SSS图像\n&emsp;&emsp;对于SSS图像，收集了14个类别的639张SSS图像。有34架飞机和240张沉船图片;这些只用于测试。在其他12个类别中，我们选择合适的图像作为样式图像数据集。\n\n&nbsp;\n\n#### 光学图像\n* 使用航拍图片,因为视图上更类似SSS。\n* 航空图像数据集：DOTA, UCAS-AOD, NWPU VHR-10, RSOD-Dataset and NWPU RESISC45。\n\n&nbsp;\n\n### 伪样本合成\n* 风格迁移网络采用VGG-19作为编码器;解码器是反向编码器。\n* 在分类部分，使用ResNet-34作为分类网络。\n* 交替训练风格迁移网络部分和分类部分。在风格转换部分，从零开始训练编码器和解码器，目标是最小化重构和感知损失的总和。使用SGD作为优化器来最小化交叉熵损失。\n* 用了一个新参数来控制噪声级，将激活值产生的噪声背景乘以$\\theta$。\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/03/1f97de18f3027647.png\" width=\"800\"/></div>\n\n&nbsp;\n\n### 实验和结果\n* 将数据集分为三类目标:飞机、船舶和其他;最后一个包含了26个类的样本，保证了在训练过程中具有丰富的负类。\n* 平均精度(即每类平均精度)的测量方法如下:\n$$\na c c_{m e a n}=\\frac{1}{\\|k\\|} \\sum_{c=1}^{\\|k\\|} \\frac{\\text { correct predictions in } c}{\\text { total samples in } c}\n$$\n* 利用伪样本合成方法，对34幅真实飞机SSS图像中的21幅进行了正确识别，对240幅真实沉船SSS图像中的212幅进行了正确识别。PhotoWCT的伪样本合成也很好，识别出了一半的飞机图像和203个沉船。基于WCT的方法表现较差，而没有风格迁移的分类器未能识别大部分飞机SSS图像。\n<div align=center><img src=\"https://i.bmp.ovh/imgs/2021/03/69732a7eba4011c3.png\" width=\"1000\"/></div>\n\n&nbsp;\n\n### 深入分析\n* 方法之所以有效，是因为在迁移学习过程中，目标域和源域越接近，知识迁移效果越好。因此，伪样本(源域)的分布与真实SSS图像(目标域)的分布越接近，知识从源向目标域的传递就越有效。此方法的关键特点是上采样方法减少了区域偏差。\n* 进一步分析了不同噪声级控制参数$\\theta$的影响。实验中，当$\\theta<0.4$，所有的PhotoWCT结果非常相似;因此，稍微有点噪音是可以接受的。$\\theta>0.5$时，性能提高，$\\theta=0.7$时在平均和全局精度方面提供了最好的结果。对$\\theta=1.0$，性能下降，因为解码过程中更多的噪声破坏了内容信息，降低了不同类别样本的可区分性。\n\n&nbsp;\n\n## Discussion\n\n&emsp;&emsp;零样本学习是迁移学习的一个极端例子。在迁移学习过程中，所有的源域和目标域数据都是可用的，因此，我们可以学习一个投影函数，将一幅图像投影到一个特征空间中，在这个空间中没有域偏向，然后完成迁移。然而，在处理零样本SSS图像分类问题时，没有目标域数据，没有投影函数是可以学习的。但是，既然我们知道目标域数据应该是什么样子，那么我们生成的源域数据就非常类似于目标域的数据，这代表了零样本学习问题的最佳解决方案。\n\n&nbsp;\n\n## Conclusion\n&emsp;&emsp;在本文中，主要研究没有真实的SSS图像来训练DNN的零样本学习图像分类问题。针对没有标记样本的类别，提出了一种固定的风格转换方法来合成伪样本，通过伪样本的合成，将零拍学习问题转化为传统的监督学习问题，利用训练好的DNN对真实SSS图像进行分类。通过源域数据的精心选择和样式传递方法的改进，实际SSS图像的识别率达到80%以上。\n\n\n","tags":["CV"],"categories":["论文阅读"]},{"title":"First Shot","url":"/2021/03/16/first/","content":"\n<center>Long long ago, a boy want to build his own website...</center>\n<!--more-->\n\n\n\n## 网站架构\n[Hexo](https://hexo.io/zh-cn/) + [Github](https://github.com/)\n\n## 主题\n[NexT.Muse](https://theme-next.js.org/muse/)\n\n## 公式插件\n[网站公式插件](https://www.jianshu.com/p/8424da4dd673)\n\n## 网站图标\n[图标](https://blog.csdn.net/Olivia_Vang/article/details/92976637?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-0&spm=1001.2101.3001.4242)\n\n## 评论插件\n[livere](https://livere.com/)\n\n## 字数统计与阅览时间插件\n[symbols-count-time](https://github.com/theme-next/hexo-symbols-count-time)\n\n## 访问与阅览次数统计插件\n[访问量和阅览次数](https://blog.csdn.net/baidu_34310405/article/details/102665373)\n\n## 图床\n[图床](https://imgurl.org/)\n\n## 鼠标点击特效\n[鼠标点击特效](https://yuanyuspace.cn/2021/04/01/clickeffect/#more)\n\n&nbsp;\n\n## Blogroll\nThanks to Mr.Wang, who helps me a lot to finish this website's construcion.  \n[gaoshi_boy's blog](https://wanglindong.com)\n","tags":["好玩的"],"categories":["网站建设"]}]